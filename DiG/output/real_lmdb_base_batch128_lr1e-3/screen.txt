Transform = 
Resize(size=(32, 128), interpolation=bicubic, max_size=None, antialias=None)
ToTensor()
Normalize(mean=0.5, std=0.5)
---------------------------
Number of the class = 97
Transform = 
Resize(size=(32, 128), interpolation=bicubic, max_size=None, antialias=None)
ToTensor()
Normalize(mean=0.5, std=0.5)
---------------------------
Number of the class = 97
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fb9490e3d30>
Patch size = (4, 4)
Load ckpt from checkpoint/pretrain/checkpoint-9.pth
Load state_dict by model_key = model
Weights of RecModel not initialized from pretrained model: ['encoder.norm.weight', 'encoder.norm.bias', 'decoder.trg_word_emb.weight', 'decoder.position_enc.position_table', 'decoder.layer_stack.0.self_attn.linear_q.weight', 'decoder.layer_stack.0.self_attn.linear_k.weight', 'decoder.layer_stack.0.self_attn.linear_v.weight', 'decoder.layer_stack.0.self_attn.fc.weight', 'decoder.layer_stack.0.norm1.weight', 'decoder.layer_stack.0.norm1.bias', 'decoder.layer_stack.0.norm2.weight', 'decoder.layer_stack.0.norm2.bias', 'decoder.layer_stack.0.norm3.weight', 'decoder.layer_stack.0.norm3.bias', 'decoder.layer_stack.0.enc_attn.linear_q.weight', 'decoder.layer_stack.0.enc_attn.linear_k.weight', 'decoder.layer_stack.0.enc_attn.linear_v.weight', 'decoder.layer_stack.0.enc_attn.fc.weight', 'decoder.layer_stack.0.mlp.w_1.weight', 'decoder.layer_stack.0.mlp.w_1.bias', 'decoder.layer_stack.0.mlp.w_2.weight', 'decoder.layer_stack.0.mlp.w_2.bias', 'decoder.layer_stack.1.self_attn.linear_q.weight', 'decoder.layer_stack.1.self_attn.linear_k.weight', 'decoder.layer_stack.1.self_attn.linear_v.weight', 'decoder.layer_stack.1.self_attn.fc.weight', 'decoder.layer_stack.1.norm1.weight', 'decoder.layer_stack.1.norm1.bias', 'decoder.layer_stack.1.norm2.weight', 'decoder.layer_stack.1.norm2.bias', 'decoder.layer_stack.1.norm3.weight', 'decoder.layer_stack.1.norm3.bias', 'decoder.layer_stack.1.enc_attn.linear_q.weight', 'decoder.layer_stack.1.enc_attn.linear_k.weight', 'decoder.layer_stack.1.enc_attn.linear_v.weight', 'decoder.layer_stack.1.enc_attn.fc.weight', 'decoder.layer_stack.1.mlp.w_1.weight', 'decoder.layer_stack.1.mlp.w_1.bias', 'decoder.layer_stack.1.mlp.w_2.weight', 'decoder.layer_stack.1.mlp.w_2.bias', 'decoder.layer_stack.2.self_attn.linear_q.weight', 'decoder.layer_stack.2.self_attn.linear_k.weight', 'decoder.layer_stack.2.self_attn.linear_v.weight', 'decoder.layer_stack.2.self_attn.fc.weight', 'decoder.layer_stack.2.norm1.weight', 'decoder.layer_stack.2.norm1.bias', 'decoder.layer_stack.2.norm2.weight', 'decoder.layer_stack.2.norm2.bias', 'decoder.layer_stack.2.norm3.weight', 'decoder.layer_stack.2.norm3.bias', 'decoder.layer_stack.2.enc_attn.linear_q.weight', 'decoder.layer_stack.2.enc_attn.linear_k.weight', 'decoder.layer_stack.2.enc_attn.linear_v.weight', 'decoder.layer_stack.2.enc_attn.fc.weight', 'decoder.layer_stack.2.mlp.w_1.weight', 'decoder.layer_stack.2.mlp.w_1.bias', 'decoder.layer_stack.2.mlp.w_2.weight', 'decoder.layer_stack.2.mlp.w_2.bias', 'decoder.layer_stack.3.self_attn.linear_q.weight', 'decoder.layer_stack.3.self_attn.linear_k.weight', 'decoder.layer_stack.3.self_attn.linear_v.weight', 'decoder.layer_stack.3.self_attn.fc.weight', 'decoder.layer_stack.3.norm1.weight', 'decoder.layer_stack.3.norm1.bias', 'decoder.layer_stack.3.norm2.weight', 'decoder.layer_stack.3.norm2.bias', 'decoder.layer_stack.3.norm3.weight', 'decoder.layer_stack.3.norm3.bias', 'decoder.layer_stack.3.enc_attn.linear_q.weight', 'decoder.layer_stack.3.enc_attn.linear_k.weight', 'decoder.layer_stack.3.enc_attn.linear_v.weight', 'decoder.layer_stack.3.enc_attn.fc.weight', 'decoder.layer_stack.3.mlp.w_1.weight', 'decoder.layer_stack.3.mlp.w_1.bias', 'decoder.layer_stack.3.mlp.w_2.weight', 'decoder.layer_stack.3.mlp.w_2.bias', 'decoder.layer_stack.4.self_attn.linear_q.weight', 'decoder.layer_stack.4.self_attn.linear_k.weight', 'decoder.layer_stack.4.self_attn.linear_v.weight', 'decoder.layer_stack.4.self_attn.fc.weight', 'decoder.layer_stack.4.norm1.weight', 'decoder.layer_stack.4.norm1.bias', 'decoder.layer_stack.4.norm2.weight', 'decoder.layer_stack.4.norm2.bias', 'decoder.layer_stack.4.norm3.weight', 'decoder.layer_stack.4.norm3.bias', 'decoder.layer_stack.4.enc_attn.linear_q.weight', 'decoder.layer_stack.4.enc_attn.linear_k.weight', 'decoder.layer_stack.4.enc_attn.linear_v.weight', 'decoder.layer_stack.4.enc_attn.fc.weight', 'decoder.layer_stack.4.mlp.w_1.weight', 'decoder.layer_stack.4.mlp.w_1.bias', 'decoder.layer_stack.4.mlp.w_2.weight', 'decoder.layer_stack.4.mlp.w_2.bias', 'decoder.layer_stack.5.self_attn.linear_q.weight', 'decoder.layer_stack.5.self_attn.linear_k.weight', 'decoder.layer_stack.5.self_attn.linear_v.weight', 'decoder.layer_stack.5.self_attn.fc.weight', 'decoder.layer_stack.5.norm1.weight', 'decoder.layer_stack.5.norm1.bias', 'decoder.layer_stack.5.norm2.weight', 'decoder.layer_stack.5.norm2.bias', 'decoder.layer_stack.5.norm3.weight', 'decoder.layer_stack.5.norm3.bias', 'decoder.layer_stack.5.enc_attn.linear_q.weight', 'decoder.layer_stack.5.enc_attn.linear_k.weight', 'decoder.layer_stack.5.enc_attn.linear_v.weight', 'decoder.layer_stack.5.enc_attn.fc.weight', 'decoder.layer_stack.5.mlp.w_1.weight', 'decoder.layer_stack.5.mlp.w_1.bias', 'decoder.layer_stack.5.mlp.w_2.weight', 'decoder.layer_stack.5.mlp.w_2.bias', 'decoder.layer_norm.weight', 'decoder.layer_norm.bias', 'decoder.classifier.weight', 'decoder.classifier.bias', 'linear_norm.0.weight', 'linear_norm.0.bias', 'linear_norm.1.weight', 'linear_norm.1.bias', 'patch_embed.proj.weight', 'patch_embed.proj.bias']
Weights from pretrained model not used in RecModel: ['momentum_encoder.mask_token', 'momentum_encoder.patch_embed.proj.weight', 'momentum_encoder.patch_embed.proj.bias', 'momentum_encoder.blocks.0.norm1.weight', 'momentum_encoder.blocks.0.norm1.bias', 'momentum_encoder.blocks.0.attn.q_bias', 'momentum_encoder.blocks.0.attn.v_bias', 'momentum_encoder.blocks.0.attn.qkv.weight', 'momentum_encoder.blocks.0.attn.proj.weight', 'momentum_encoder.blocks.0.attn.proj.bias', 'momentum_encoder.blocks.0.norm2.weight', 'momentum_encoder.blocks.0.norm2.bias', 'momentum_encoder.blocks.0.mlp.fc1.weight', 'momentum_encoder.blocks.0.mlp.fc1.bias', 'momentum_encoder.blocks.0.mlp.fc2.weight', 'momentum_encoder.blocks.0.mlp.fc2.bias', 'momentum_encoder.blocks.1.norm1.weight', 'momentum_encoder.blocks.1.norm1.bias', 'momentum_encoder.blocks.1.attn.q_bias', 'momentum_encoder.blocks.1.attn.v_bias', 'momentum_encoder.blocks.1.attn.qkv.weight', 'momentum_encoder.blocks.1.attn.proj.weight', 'momentum_encoder.blocks.1.attn.proj.bias', 'momentum_encoder.blocks.1.norm2.weight', 'momentum_encoder.blocks.1.norm2.bias', 'momentum_encoder.blocks.1.mlp.fc1.weight', 'momentum_encoder.blocks.1.mlp.fc1.bias', 'momentum_encoder.blocks.1.mlp.fc2.weight', 'momentum_encoder.blocks.1.mlp.fc2.bias', 'momentum_encoder.blocks.2.norm1.weight', 'momentum_encoder.blocks.2.norm1.bias', 'momentum_encoder.blocks.2.attn.q_bias', 'momentum_encoder.blocks.2.attn.v_bias', 'momentum_encoder.blocks.2.attn.qkv.weight', 'momentum_encoder.blocks.2.attn.proj.weight', 'momentum_encoder.blocks.2.attn.proj.bias', 'momentum_encoder.blocks.2.norm2.weight', 'momentum_encoder.blocks.2.norm2.bias', 'momentum_encoder.blocks.2.mlp.fc1.weight', 'momentum_encoder.blocks.2.mlp.fc1.bias', 'momentum_encoder.blocks.2.mlp.fc2.weight', 'momentum_encoder.blocks.2.mlp.fc2.bias', 'momentum_encoder.blocks.3.norm1.weight', 'momentum_encoder.blocks.3.norm1.bias', 'momentum_encoder.blocks.3.attn.q_bias', 'momentum_encoder.blocks.3.attn.v_bias', 'momentum_encoder.blocks.3.attn.qkv.weight', 'momentum_encoder.blocks.3.attn.proj.weight', 'momentum_encoder.blocks.3.attn.proj.bias', 'momentum_encoder.blocks.3.norm2.weight', 'momentum_encoder.blocks.3.norm2.bias', 'momentum_encoder.blocks.3.mlp.fc1.weight', 'momentum_encoder.blocks.3.mlp.fc1.bias', 'momentum_encoder.blocks.3.mlp.fc2.weight', 'momentum_encoder.blocks.3.mlp.fc2.bias', 'momentum_encoder.blocks.4.norm1.weight', 'momentum_encoder.blocks.4.norm1.bias', 'momentum_encoder.blocks.4.attn.q_bias', 'momentum_encoder.blocks.4.attn.v_bias', 'momentum_encoder.blocks.4.attn.qkv.weight', 'momentum_encoder.blocks.4.attn.proj.weight', 'momentum_encoder.blocks.4.attn.proj.bias', 'momentum_encoder.blocks.4.norm2.weight', 'momentum_encoder.blocks.4.norm2.bias', 'momentum_encoder.blocks.4.mlp.fc1.weight', 'momentum_encoder.blocks.4.mlp.fc1.bias', 'momentum_encoder.blocks.4.mlp.fc2.weight', 'momentum_encoder.blocks.4.mlp.fc2.bias', 'momentum_encoder.blocks.5.norm1.weight', 'momentum_encoder.blocks.5.norm1.bias', 'momentum_encoder.blocks.5.attn.q_bias', 'momentum_encoder.blocks.5.attn.v_bias', 'momentum_encoder.blocks.5.attn.qkv.weight', 'momentum_encoder.blocks.5.attn.proj.weight', 'momentum_encoder.blocks.5.attn.proj.bias', 'momentum_encoder.blocks.5.norm2.weight', 'momentum_encoder.blocks.5.norm2.bias', 'momentum_encoder.blocks.5.mlp.fc1.weight', 'momentum_encoder.blocks.5.mlp.fc1.bias', 'momentum_encoder.blocks.5.mlp.fc2.weight', 'momentum_encoder.blocks.5.mlp.fc2.bias', 'momentum_encoder.blocks.6.norm1.weight', 'momentum_encoder.blocks.6.norm1.bias', 'momentum_encoder.blocks.6.attn.q_bias', 'momentum_encoder.blocks.6.attn.v_bias', 'momentum_encoder.blocks.6.attn.qkv.weight', 'momentum_encoder.blocks.6.attn.proj.weight', 'momentum_encoder.blocks.6.attn.proj.bias', 'momentum_encoder.blocks.6.norm2.weight', 'momentum_encoder.blocks.6.norm2.bias', 'momentum_encoder.blocks.6.mlp.fc1.weight', 'momentum_encoder.blocks.6.mlp.fc1.bias', 'momentum_encoder.blocks.6.mlp.fc2.weight', 'momentum_encoder.blocks.6.mlp.fc2.bias', 'momentum_encoder.blocks.7.norm1.weight', 'momentum_encoder.blocks.7.norm1.bias', 'momentum_encoder.blocks.7.attn.q_bias', 'momentum_encoder.blocks.7.attn.v_bias', 'momentum_encoder.blocks.7.attn.qkv.weight', 'momentum_encoder.blocks.7.attn.proj.weight', 'momentum_encoder.blocks.7.attn.proj.bias', 'momentum_encoder.blocks.7.norm2.weight', 'momentum_encoder.blocks.7.norm2.bias', 'momentum_encoder.blocks.7.mlp.fc1.weight', 'momentum_encoder.blocks.7.mlp.fc1.bias', 'momentum_encoder.blocks.7.mlp.fc2.weight', 'momentum_encoder.blocks.7.mlp.fc2.bias', 'momentum_encoder.blocks.8.norm1.weight', 'momentum_encoder.blocks.8.norm1.bias', 'momentum_encoder.blocks.8.attn.q_bias', 'momentum_encoder.blocks.8.attn.v_bias', 'momentum_encoder.blocks.8.attn.qkv.weight', 'momentum_encoder.blocks.8.attn.proj.weight', 'momentum_encoder.blocks.8.attn.proj.bias', 'momentum_encoder.blocks.8.norm2.weight', 'momentum_encoder.blocks.8.norm2.bias', 'momentum_encoder.blocks.8.mlp.fc1.weight', 'momentum_encoder.blocks.8.mlp.fc1.bias', 'momentum_encoder.blocks.8.mlp.fc2.weight', 'momentum_encoder.blocks.8.mlp.fc2.bias', 'momentum_encoder.blocks.9.norm1.weight', 'momentum_encoder.blocks.9.norm1.bias', 'momentum_encoder.blocks.9.attn.q_bias', 'momentum_encoder.blocks.9.attn.v_bias', 'momentum_encoder.blocks.9.attn.qkv.weight', 'momentum_encoder.blocks.9.attn.proj.weight', 'momentum_encoder.blocks.9.attn.proj.bias', 'momentum_encoder.blocks.9.norm2.weight', 'momentum_encoder.blocks.9.norm2.bias', 'momentum_encoder.blocks.9.mlp.fc1.weight', 'momentum_encoder.blocks.9.mlp.fc1.bias', 'momentum_encoder.blocks.9.mlp.fc2.weight', 'momentum_encoder.blocks.9.mlp.fc2.bias', 'momentum_encoder.blocks.10.norm1.weight', 'momentum_encoder.blocks.10.norm1.bias', 'momentum_encoder.blocks.10.attn.q_bias', 'momentum_encoder.blocks.10.attn.v_bias', 'momentum_encoder.blocks.10.attn.qkv.weight', 'momentum_encoder.blocks.10.attn.proj.weight', 'momentum_encoder.blocks.10.attn.proj.bias', 'momentum_encoder.blocks.10.norm2.weight', 'momentum_encoder.blocks.10.norm2.bias', 'momentum_encoder.blocks.10.mlp.fc1.weight', 'momentum_encoder.blocks.10.mlp.fc1.bias', 'momentum_encoder.blocks.10.mlp.fc2.weight', 'momentum_encoder.blocks.10.mlp.fc2.bias', 'momentum_encoder.blocks.11.norm1.weight', 'momentum_encoder.blocks.11.norm1.bias', 'momentum_encoder.blocks.11.attn.q_bias', 'momentum_encoder.blocks.11.attn.v_bias', 'momentum_encoder.blocks.11.attn.qkv.weight', 'momentum_encoder.blocks.11.attn.proj.weight', 'momentum_encoder.blocks.11.attn.proj.bias', 'momentum_encoder.blocks.11.norm2.weight', 'momentum_encoder.blocks.11.norm2.bias', 'momentum_encoder.blocks.11.mlp.fc1.weight', 'momentum_encoder.blocks.11.mlp.fc1.bias', 'momentum_encoder.blocks.11.mlp.fc2.weight', 'momentum_encoder.blocks.11.mlp.fc2.bias', 'encoder_projection_layer.0.weight', 'encoder_projection_layer.1.weight', 'encoder_projection_layer.1.bias', 'encoder_projection_layer.1.running_mean', 'encoder_projection_layer.1.running_var', 'encoder_projection_layer.1.num_batches_tracked', 'encoder_projection_layer.3.weight', 'encoder_projection_layer.4.weight', 'encoder_projection_layer.4.bias', 'encoder_projection_layer.4.running_mean', 'encoder_projection_layer.4.running_var', 'encoder_projection_layer.4.num_batches_tracked', 'encoder_projection_layer.6.weight', 'encoder_projection_layer.7.running_mean', 'encoder_projection_layer.7.running_var', 'encoder_projection_layer.7.num_batches_tracked', 'momentum_projection_layer.0.weight', 'momentum_projection_layer.1.weight', 'momentum_projection_layer.1.bias', 'momentum_projection_layer.1.running_mean', 'momentum_projection_layer.1.running_var', 'momentum_projection_layer.1.num_batches_tracked', 'momentum_projection_layer.3.weight', 'momentum_projection_layer.4.weight', 'momentum_projection_layer.4.bias', 'momentum_projection_layer.4.running_mean', 'momentum_projection_layer.4.running_var', 'momentum_projection_layer.4.num_batches_tracked', 'momentum_projection_layer.6.weight', 'momentum_projection_layer.7.running_mean', 'momentum_projection_layer.7.running_var', 'momentum_projection_layer.7.num_batches_tracked', 'predictor.0.weight', 'predictor.1.weight', 'predictor.1.bias', 'predictor.1.running_mean', 'predictor.1.running_var', 'predictor.1.num_batches_tracked', 'predictor.3.weight', 'predictor.4.running_mean', 'predictor.4.running_var', 'predictor.4.num_batches_tracked', 'pix_projector.0.weight', 'pix_projector.1.weight', 'pix_projector.1.bias', 'pix_projector.1.running_mean', 'pix_projector.1.running_var', 'pix_projector.1.num_batches_tracked', 'pix_projector.3.weight', 'pix_projector.4.weight', 'pix_projector.4.bias', 'pix_projector.4.running_mean', 'pix_projector.4.running_var', 'pix_projector.4.num_batches_tracked', 'pix_projector.6.weight', 'pix_projector.7.running_mean', 'pix_projector.7.running_var', 'pix_projector.7.num_batches_tracked', 'pix_projector_m.0.weight', 'pix_projector_m.1.weight', 'pix_projector_m.1.bias', 'pix_projector_m.1.running_mean', 'pix_projector_m.1.running_var', 'pix_projector_m.1.num_batches_tracked', 'pix_projector_m.3.weight', 'pix_projector_m.4.weight', 'pix_projector_m.4.bias', 'pix_projector_m.4.running_mean', 'pix_projector_m.4.running_var', 'pix_projector_m.4.num_batches_tracked', 'pix_projector_m.6.weight', 'pix_projector_m.7.running_mean', 'pix_projector_m.7.running_var', 'pix_projector_m.7.num_batches_tracked', 'pix_decoder.0.weight', 'pix_decoder.1.weight', 'pix_decoder.2.weight', 'pix_decoder.2.bias', 'pix_decoder.4.weight', 'pix_decoder.4.bias']
Model = RecModel(
  (encoder): PretrainVisionTransformerEncoder(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (drop_path): DropPath(p=0.00909090880304575)
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (drop_path): DropPath(p=0.0181818176060915)
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (drop_path): DropPath(p=0.027272727340459824)
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (drop_path): DropPath(p=0.036363635212183)
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (drop_path): DropPath(p=0.045454543083906174)
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (drop_path): DropPath(p=0.054545458406209946)
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (drop_path): DropPath(p=0.06363636255264282)
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (drop_path): DropPath(p=0.0727272778749466)
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (drop_path): DropPath(p=0.08181818574666977)
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (drop_path): DropPath(p=0.09090909361839294)
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (drop_path): DropPath(p=0.10000000149011612)
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
    (head): Identity()
  )
  (decoder): TFDecoder(
    (trg_word_emb): Embedding(98, 512)
    (position_enc): PositionalEncoding()
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_stack): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=False)
          (linear_k): Linear(in_features=512, out_features=512, bias=False)
          (linear_v): Linear(in_features=512, out_features=512, bias=False)
          (fc): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (enc_attn): MultiHeadAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=False)
          (linear_k): Linear(in_features=512, out_features=512, bias=False)
          (linear_v): Linear(in_features=512, out_features=512, bias=False)
          (fc): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (mlp): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=256, bias=True)
          (w_2): Linear(in_features=256, out_features=512, bias=True)
          (act): GELU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=False)
          (linear_k): Linear(in_features=512, out_features=512, bias=False)
          (linear_v): Linear(in_features=512, out_features=512, bias=False)
          (fc): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (enc_attn): MultiHeadAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=False)
          (linear_k): Linear(in_features=512, out_features=512, bias=False)
          (linear_v): Linear(in_features=512, out_features=512, bias=False)
          (fc): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (mlp): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=256, bias=True)
          (w_2): Linear(in_features=256, out_features=512, bias=True)
          (act): GELU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=False)
          (linear_k): Linear(in_features=512, out_features=512, bias=False)
          (linear_v): Linear(in_features=512, out_features=512, bias=False)
          (fc): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (enc_attn): MultiHeadAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=False)
          (linear_k): Linear(in_features=512, out_features=512, bias=False)
          (linear_v): Linear(in_features=512, out_features=512, bias=False)
          (fc): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (mlp): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=256, bias=True)
          (w_2): Linear(in_features=256, out_features=512, bias=True)
          (act): GELU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=False)
          (linear_k): Linear(in_features=512, out_features=512, bias=False)
          (linear_v): Linear(in_features=512, out_features=512, bias=False)
          (fc): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (enc_attn): MultiHeadAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=False)
          (linear_k): Linear(in_features=512, out_features=512, bias=False)
          (linear_v): Linear(in_features=512, out_features=512, bias=False)
          (fc): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (mlp): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=256, bias=True)
          (w_2): Linear(in_features=256, out_features=512, bias=True)
          (act): GELU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=False)
          (linear_k): Linear(in_features=512, out_features=512, bias=False)
          (linear_v): Linear(in_features=512, out_features=512, bias=False)
          (fc): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (enc_attn): MultiHeadAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=False)
          (linear_k): Linear(in_features=512, out_features=512, bias=False)
          (linear_v): Linear(in_features=512, out_features=512, bias=False)
          (fc): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (mlp): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=256, bias=True)
          (w_2): Linear(in_features=256, out_features=512, bias=True)
          (act): GELU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=False)
          (linear_k): Linear(in_features=512, out_features=512, bias=False)
          (linear_v): Linear(in_features=512, out_features=512, bias=False)
          (fc): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (enc_attn): MultiHeadAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=False)
          (linear_k): Linear(in_features=512, out_features=512, bias=False)
          (linear_v): Linear(in_features=512, out_features=512, bias=False)
          (fc): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.1, inplace=False)
          (proj_drop): Dropout(p=0.1, inplace=False)
        )
        (mlp): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=256, bias=True)
          (w_2): Linear(in_features=256, out_features=512, bias=True)
          (act): GELU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    (classifier): Linear(in_features=512, out_features=97, bias=True)
  )
  (linear_norm): Sequential(
    (0): Linear(in_features=384, out_features=512, bias=True)
    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
)
number of params: 35786849
LR = 0.00050000
Batch size = 128
Update frequent = 1
Number of training examples = 12062
Number of training training per epoch = 94
Assigned values = [0.023757264018058777, 0.03167635202407837, 0.04223513603210449, 0.056313514709472656, 0.07508468627929688, 0.1001129150390625, 0.13348388671875, 0.177978515625, 0.2373046875, 0.31640625, 0.421875, 0.5625, 0.75, 1.0]
Skip weight decay list:  {'encoder.cls_token', 'encoder.pos_embed'}
these layers are fixed during training:  []
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "encoder.mask_token",
      "encoder.patch_embed.proj.weight"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "encoder.patch_embed.proj.bias"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "encoder.blocks.0.norm1.weight",
      "encoder.blocks.0.norm1.bias",
      "encoder.blocks.0.attn.q_bias",
      "encoder.blocks.0.attn.v_bias",
      "encoder.blocks.0.attn.proj.bias",
      "encoder.blocks.0.norm2.weight",
      "encoder.blocks.0.norm2.bias",
      "encoder.blocks.0.mlp.fc1.bias",
      "encoder.blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "encoder.blocks.0.attn.qkv.weight",
      "encoder.blocks.0.attn.proj.weight",
      "encoder.blocks.0.mlp.fc1.weight",
      "encoder.blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "encoder.blocks.1.norm1.weight",
      "encoder.blocks.1.norm1.bias",
      "encoder.blocks.1.attn.q_bias",
      "encoder.blocks.1.attn.v_bias",
      "encoder.blocks.1.attn.proj.bias",
      "encoder.blocks.1.norm2.weight",
      "encoder.blocks.1.norm2.bias",
      "encoder.blocks.1.mlp.fc1.bias",
      "encoder.blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "encoder.blocks.1.attn.qkv.weight",
      "encoder.blocks.1.attn.proj.weight",
      "encoder.blocks.1.mlp.fc1.weight",
      "encoder.blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "encoder.blocks.2.norm1.weight",
      "encoder.blocks.2.norm1.bias",
      "encoder.blocks.2.attn.q_bias",
      "encoder.blocks.2.attn.v_bias",
      "encoder.blocks.2.attn.proj.bias",
      "encoder.blocks.2.norm2.weight",
      "encoder.blocks.2.norm2.bias",
      "encoder.blocks.2.mlp.fc1.bias",
      "encoder.blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "encoder.blocks.2.attn.qkv.weight",
      "encoder.blocks.2.attn.proj.weight",
      "encoder.blocks.2.mlp.fc1.weight",
      "encoder.blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "encoder.blocks.3.norm1.weight",
      "encoder.blocks.3.norm1.bias",
      "encoder.blocks.3.attn.q_bias",
      "encoder.blocks.3.attn.v_bias",
      "encoder.blocks.3.attn.proj.bias",
      "encoder.blocks.3.norm2.weight",
      "encoder.blocks.3.norm2.bias",
      "encoder.blocks.3.mlp.fc1.bias",
      "encoder.blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "encoder.blocks.3.attn.qkv.weight",
      "encoder.blocks.3.attn.proj.weight",
      "encoder.blocks.3.mlp.fc1.weight",
      "encoder.blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "encoder.blocks.4.norm1.weight",
      "encoder.blocks.4.norm1.bias",
      "encoder.blocks.4.attn.q_bias",
      "encoder.blocks.4.attn.v_bias",
      "encoder.blocks.4.attn.proj.bias",
      "encoder.blocks.4.norm2.weight",
      "encoder.blocks.4.norm2.bias",
      "encoder.blocks.4.mlp.fc1.bias",
      "encoder.blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "encoder.blocks.4.attn.qkv.weight",
      "encoder.blocks.4.attn.proj.weight",
      "encoder.blocks.4.mlp.fc1.weight",
      "encoder.blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "encoder.blocks.5.norm1.weight",
      "encoder.blocks.5.norm1.bias",
      "encoder.blocks.5.attn.q_bias",
      "encoder.blocks.5.attn.v_bias",
      "encoder.blocks.5.attn.proj.bias",
      "encoder.blocks.5.norm2.weight",
      "encoder.blocks.5.norm2.bias",
      "encoder.blocks.5.mlp.fc1.bias",
      "encoder.blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "encoder.blocks.5.attn.qkv.weight",
      "encoder.blocks.5.attn.proj.weight",
      "encoder.blocks.5.mlp.fc1.weight",
      "encoder.blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "encoder.blocks.6.norm1.weight",
      "encoder.blocks.6.norm1.bias",
      "encoder.blocks.6.attn.q_bias",
      "encoder.blocks.6.attn.v_bias",
      "encoder.blocks.6.attn.proj.bias",
      "encoder.blocks.6.norm2.weight",
      "encoder.blocks.6.norm2.bias",
      "encoder.blocks.6.mlp.fc1.bias",
      "encoder.blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "encoder.blocks.6.attn.qkv.weight",
      "encoder.blocks.6.attn.proj.weight",
      "encoder.blocks.6.mlp.fc1.weight",
      "encoder.blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "encoder.blocks.7.norm1.weight",
      "encoder.blocks.7.norm1.bias",
      "encoder.blocks.7.attn.q_bias",
      "encoder.blocks.7.attn.v_bias",
      "encoder.blocks.7.attn.proj.bias",
      "encoder.blocks.7.norm2.weight",
      "encoder.blocks.7.norm2.bias",
      "encoder.blocks.7.mlp.fc1.bias",
      "encoder.blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "encoder.blocks.7.attn.qkv.weight",
      "encoder.blocks.7.attn.proj.weight",
      "encoder.blocks.7.mlp.fc1.weight",
      "encoder.blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "encoder.blocks.8.norm1.weight",
      "encoder.blocks.8.norm1.bias",
      "encoder.blocks.8.attn.q_bias",
      "encoder.blocks.8.attn.v_bias",
      "encoder.blocks.8.attn.proj.bias",
      "encoder.blocks.8.norm2.weight",
      "encoder.blocks.8.norm2.bias",
      "encoder.blocks.8.mlp.fc1.bias",
      "encoder.blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.31640625
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "encoder.blocks.8.attn.qkv.weight",
      "encoder.blocks.8.attn.proj.weight",
      "encoder.blocks.8.mlp.fc1.weight",
      "encoder.blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.31640625
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "encoder.blocks.9.norm1.weight",
      "encoder.blocks.9.norm1.bias",
      "encoder.blocks.9.attn.q_bias",
      "encoder.blocks.9.attn.v_bias",
      "encoder.blocks.9.attn.proj.bias",
      "encoder.blocks.9.norm2.weight",
      "encoder.blocks.9.norm2.bias",
      "encoder.blocks.9.mlp.fc1.bias",
      "encoder.blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.421875
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "encoder.blocks.9.attn.qkv.weight",
      "encoder.blocks.9.attn.proj.weight",
      "encoder.blocks.9.mlp.fc1.weight",
      "encoder.blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.421875
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "encoder.blocks.10.norm1.weight",
      "encoder.blocks.10.norm1.bias",
      "encoder.blocks.10.attn.q_bias",
      "encoder.blocks.10.attn.v_bias",
      "encoder.blocks.10.attn.proj.bias",
      "encoder.blocks.10.norm2.weight",
      "encoder.blocks.10.norm2.bias",
      "encoder.blocks.10.mlp.fc1.bias",
      "encoder.blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.5625
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "encoder.blocks.10.attn.qkv.weight",
      "encoder.blocks.10.attn.proj.weight",
      "encoder.blocks.10.mlp.fc1.weight",
      "encoder.blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.5625
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "encoder.blocks.11.norm1.weight",
      "encoder.blocks.11.norm1.bias",
      "encoder.blocks.11.attn.q_bias",
      "encoder.blocks.11.attn.v_bias",
      "encoder.blocks.11.attn.proj.bias",
      "encoder.blocks.11.norm2.weight",
      "encoder.blocks.11.norm2.bias",
      "encoder.blocks.11.mlp.fc1.bias",
      "encoder.blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.75
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "encoder.blocks.11.attn.qkv.weight",
      "encoder.blocks.11.attn.proj.weight",
      "encoder.blocks.11.mlp.fc1.weight",
      "encoder.blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.75
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "encoder.norm.weight",
      "encoder.norm.bias",
      "decoder.layer_stack.0.norm1.weight",
      "decoder.layer_stack.0.norm1.bias",
      "decoder.layer_stack.0.norm2.weight",
      "decoder.layer_stack.0.norm2.bias",
      "decoder.layer_stack.0.norm3.weight",
      "decoder.layer_stack.0.norm3.bias",
      "decoder.layer_stack.0.mlp.w_1.bias",
      "decoder.layer_stack.0.mlp.w_2.bias",
      "decoder.layer_stack.1.norm1.weight",
      "decoder.layer_stack.1.norm1.bias",
      "decoder.layer_stack.1.norm2.weight",
      "decoder.layer_stack.1.norm2.bias",
      "decoder.layer_stack.1.norm3.weight",
      "decoder.layer_stack.1.norm3.bias",
      "decoder.layer_stack.1.mlp.w_1.bias",
      "decoder.layer_stack.1.mlp.w_2.bias",
      "decoder.layer_stack.2.norm1.weight",
      "decoder.layer_stack.2.norm1.bias",
      "decoder.layer_stack.2.norm2.weight",
      "decoder.layer_stack.2.norm2.bias",
      "decoder.layer_stack.2.norm3.weight",
      "decoder.layer_stack.2.norm3.bias",
      "decoder.layer_stack.2.mlp.w_1.bias",
      "decoder.layer_stack.2.mlp.w_2.bias",
      "decoder.layer_stack.3.norm1.weight",
      "decoder.layer_stack.3.norm1.bias",
      "decoder.layer_stack.3.norm2.weight",
      "decoder.layer_stack.3.norm2.bias",
      "decoder.layer_stack.3.norm3.weight",
      "decoder.layer_stack.3.norm3.bias",
      "decoder.layer_stack.3.mlp.w_1.bias",
      "decoder.layer_stack.3.mlp.w_2.bias",
      "decoder.layer_stack.4.norm1.weight",
      "decoder.layer_stack.4.norm1.bias",
      "decoder.layer_stack.4.norm2.weight",
      "decoder.layer_stack.4.norm2.bias",
      "decoder.layer_stack.4.norm3.weight",
      "decoder.layer_stack.4.norm3.bias",
      "decoder.layer_stack.4.mlp.w_1.bias",
      "decoder.layer_stack.4.mlp.w_2.bias",
      "decoder.layer_stack.5.norm1.weight",
      "decoder.layer_stack.5.norm1.bias",
      "decoder.layer_stack.5.norm2.weight",
      "decoder.layer_stack.5.norm2.bias",
      "decoder.layer_stack.5.norm3.weight",
      "decoder.layer_stack.5.norm3.bias",
      "decoder.layer_stack.5.mlp.w_1.bias",
      "decoder.layer_stack.5.mlp.w_2.bias",
      "decoder.layer_norm.weight",
      "decoder.layer_norm.bias",
      "decoder.classifier.bias",
      "linear_norm.0.bias",
      "linear_norm.1.weight",
      "linear_norm.1.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "decoder.trg_word_emb.weight",
      "decoder.layer_stack.0.self_attn.linear_q.weight",
      "decoder.layer_stack.0.self_attn.linear_k.weight",
      "decoder.layer_stack.0.self_attn.linear_v.weight",
      "decoder.layer_stack.0.self_attn.fc.weight",
      "decoder.layer_stack.0.enc_attn.linear_q.weight",
      "decoder.layer_stack.0.enc_attn.linear_k.weight",
      "decoder.layer_stack.0.enc_attn.linear_v.weight",
      "decoder.layer_stack.0.enc_attn.fc.weight",
      "decoder.layer_stack.0.mlp.w_1.weight",
      "decoder.layer_stack.0.mlp.w_2.weight",
      "decoder.layer_stack.1.self_attn.linear_q.weight",
      "decoder.layer_stack.1.self_attn.linear_k.weight",
      "decoder.layer_stack.1.self_attn.linear_v.weight",
      "decoder.layer_stack.1.self_attn.fc.weight",
      "decoder.layer_stack.1.enc_attn.linear_q.weight",
      "decoder.layer_stack.1.enc_attn.linear_k.weight",
      "decoder.layer_stack.1.enc_attn.linear_v.weight",
      "decoder.layer_stack.1.enc_attn.fc.weight",
      "decoder.layer_stack.1.mlp.w_1.weight",
      "decoder.layer_stack.1.mlp.w_2.weight",
      "decoder.layer_stack.2.self_attn.linear_q.weight",
      "decoder.layer_stack.2.self_attn.linear_k.weight",
      "decoder.layer_stack.2.self_attn.linear_v.weight",
      "decoder.layer_stack.2.self_attn.fc.weight",
      "decoder.layer_stack.2.enc_attn.linear_q.weight",
      "decoder.layer_stack.2.enc_attn.linear_k.weight",
      "decoder.layer_stack.2.enc_attn.linear_v.weight",
      "decoder.layer_stack.2.enc_attn.fc.weight",
      "decoder.layer_stack.2.mlp.w_1.weight",
      "decoder.layer_stack.2.mlp.w_2.weight",
      "decoder.layer_stack.3.self_attn.linear_q.weight",
      "decoder.layer_stack.3.self_attn.linear_k.weight",
      "decoder.layer_stack.3.self_attn.linear_v.weight",
      "decoder.layer_stack.3.self_attn.fc.weight",
      "decoder.layer_stack.3.enc_attn.linear_q.weight",
      "decoder.layer_stack.3.enc_attn.linear_k.weight",
      "decoder.layer_stack.3.enc_attn.linear_v.weight",
      "decoder.layer_stack.3.enc_attn.fc.weight",
      "decoder.layer_stack.3.mlp.w_1.weight",
      "decoder.layer_stack.3.mlp.w_2.weight",
      "decoder.layer_stack.4.self_attn.linear_q.weight",
      "decoder.layer_stack.4.self_attn.linear_k.weight",
      "decoder.layer_stack.4.self_attn.linear_v.weight",
      "decoder.layer_stack.4.self_attn.fc.weight",
      "decoder.layer_stack.4.enc_attn.linear_q.weight",
      "decoder.layer_stack.4.enc_attn.linear_k.weight",
      "decoder.layer_stack.4.enc_attn.linear_v.weight",
      "decoder.layer_stack.4.enc_attn.fc.weight",
      "decoder.layer_stack.4.mlp.w_1.weight",
      "decoder.layer_stack.4.mlp.w_2.weight",
      "decoder.layer_stack.5.self_attn.linear_q.weight",
      "decoder.layer_stack.5.self_attn.linear_k.weight",
      "decoder.layer_stack.5.self_attn.linear_v.weight",
      "decoder.layer_stack.5.self_attn.fc.weight",
      "decoder.layer_stack.5.enc_attn.linear_q.weight",
      "decoder.layer_stack.5.enc_attn.linear_k.weight",
      "decoder.layer_stack.5.enc_attn.linear_v.weight",
      "decoder.layer_stack.5.enc_attn.fc.weight",
      "decoder.layer_stack.5.mlp.w_1.weight",
      "decoder.layer_stack.5.mlp.w_2.weight",
      "decoder.classifier.weight",
      "linear_norm.0.weight"
    ],
    "lr_scale": 1.0
  }
}
optimizer settings: {'lr': 0.0005, 'weight_decay': 0.0, 'eps': 1e-08, 'betas': [0.9, 0.999]}
Use step level LR scheduler!
Set warmup steps = 94
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SeqCrossEntropyLoss()
Auto resume checkpoint: 
Start training for 100 epochs
[2024-07-17 06:55:39]  Epoch: [0]  [ 0/94]  eta: 0:11:05  lr: 0.000000  min_lr: 0.000000  loss: 26.4645 (26.4645)  class_acc: 0.0000 (0.0000)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: inf (inf)  time: 7.0777  data: 6.5863  max mem: 13934
[2024-07-17 06:56:42]  Epoch: [0]  [93/94]  eta: 0:00:00  lr: 0.000500  min_lr: 0.000012  loss: 12.2116 (16.5104)  class_acc: 0.1340 (0.0460)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.5527 (inf)  time: 0.6435  data: 0.3020  max mem: 14356
Epoch: [0] Total time: 0:01:10 (0.7494 s / it)
Averaged stats: lr: 0.000500  min_lr: 0.000012  loss: 12.2116 (16.5104)  class_acc: 0.1340 (0.0460)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.5527 (inf)
[2024-07-17 06:56:48]  Epoch: [1]  [ 0/94]  eta: 0:08:40  lr: 0.000500  min_lr: 0.000012  loss: 10.4584 (10.4584)  class_acc: 0.1875 (0.1875)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.9268 (19.9268)  time: 5.5386  data: 5.1805  max mem: 14356
[2024-07-17 06:57:52]  Epoch: [1]  [93/94]  eta: 0:00:00  lr: 0.000500  min_lr: 0.000012  loss: 6.7131 (8.2187)  class_acc: 0.4477 (0.3556)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 20.0404 (20.8763)  time: 0.5149  data: 0.1715  max mem: 14356
Epoch: [1] Total time: 0:01:10 (0.7478 s / it)
Averaged stats: lr: 0.000500  min_lr: 0.000012  loss: 6.7131 (8.2187)  class_acc: 0.4477 (0.3556)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 20.0404 (20.8763)
[2024-07-17 06:57:57]  Epoch: [2]  [ 0/94]  eta: 0:07:08  lr: 0.000500  min_lr: 0.000012  loss: 6.6814 (6.6814)  class_acc: 0.4375 (0.4375)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 21.1762 (21.1762)  time: 4.5570  data: 4.1898  max mem: 14356
[2024-07-17 06:59:08]  Epoch: [2]  [93/94]  eta: 0:00:00  lr: 0.000500  min_lr: 0.000012  loss: 5.5479 (5.8318)  class_acc: 0.5285 (0.5127)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 20.0997 (19.4727)  time: 0.6085  data: 0.2636  max mem: 14356
Epoch: [2] Total time: 0:01:15 (0.8039 s / it)
Averaged stats: lr: 0.000500  min_lr: 0.000012  loss: 5.5479 (5.8318)  class_acc: 0.5285 (0.5127)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 20.0997 (19.4727)
[2024-07-17 06:59:12]  Epoch: [3]  [ 0/94]  eta: 0:06:20  lr: 0.000499  min_lr: 0.000012  loss: 5.1637 (5.1637)  class_acc: 0.5938 (0.5938)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.1221 (16.1221)  time: 4.0519  data: 3.6901  max mem: 14356
[2024-07-17 07:00:09]  Epoch: [3]  [93/94]  eta: 0:00:00  lr: 0.000499  min_lr: 0.000012  loss: 4.6333 (4.8938)  class_acc: 0.5883 (0.5775)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.9879 (17.8912)  time: 0.5974  data: 0.2541  max mem: 14356
Epoch: [3] Total time: 0:01:01 (0.6539 s / it)
Averaged stats: lr: 0.000499  min_lr: 0.000012  loss: 4.6333 (4.8938)  class_acc: 0.5883 (0.5775)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.9879 (17.8912)
[2024-07-17 07:00:17]  Epoch: [4]  [ 0/94]  eta: 0:11:44  lr: 0.000499  min_lr: 0.000012  loss: 4.8919 (4.8919)  class_acc: 0.5625 (0.5625)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 17.3390 (17.3390)  time: 7.4974  data: 7.1381  max mem: 14356
[2024-07-17 07:01:19]  Epoch: [4]  [93/94]  eta: 0:00:00  lr: 0.000498  min_lr: 0.000012  loss: 4.0694 (4.2940)  class_acc: 0.6195 (0.6100)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 17.5936 (17.7425)  time: 0.5721  data: 0.2286  max mem: 14356
Epoch: [4] Total time: 0:01:09 (0.7381 s / it)
Averaged stats: lr: 0.000498  min_lr: 0.000012  loss: 4.0694 (4.2940)  class_acc: 0.6195 (0.6100)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 17.5936 (17.7425)
[2024-07-17 07:01:23]  Test:  [ 0/16]  eta: 0:01:01  loss: 21.1198 (21.1198)  acc: 0.8021 (0.8021)  recognition_fmeasure: 0.9559 (0.9559)  time: 3.8684  data: 3.2129  max mem: 14356
[2024-07-17 07:01:29]  Test:  [10/16]  eta: 0:00:05  loss: 20.5167 (20.5167)  acc: 0.7604 (0.7604)  recognition_fmeasure: 0.8957 (0.8957)  time: 0.9381  data: 0.2922  max mem: 14356
[2024-07-17 07:01:32]  Test:  [15/16]  eta: 0:00:00  loss: 20.7807 (20.7807)  acc: 0.7670 (0.7654)  recognition_fmeasure: 0.9068 (0.9033)  time: 0.8211  data: 0.2009  max mem: 14356
Test: Total time: 0:00:13 (0.8426 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.7654 loss 20.7807 Rec_fmeasure 0.9033
Accuracy of the network on the 2941 test images: 0.7654%
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-best.pth
Max accuracy: 0.77%
[2024-07-17 07:01:38]  Epoch: [5]  [ 0/94]  eta: 0:08:07  lr: 0.000498  min_lr: 0.000012  loss: 4.3171 (4.3171)  class_acc: 0.6172 (0.6172)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 18.6113 (18.6113)  time: 5.1838  data: 4.8177  max mem: 14356
[2024-07-17 07:02:42]  Epoch: [5]  [93/94]  eta: 0:00:00  lr: 0.000497  min_lr: 0.000012  loss: 3.9088 (3.9517)  class_acc: 0.6375 (0.6367)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 17.1950 (17.0060)  time: 0.5595  data: 0.2167  max mem: 14356
Epoch: [5] Total time: 0:01:09 (0.7391 s / it)
Averaged stats: lr: 0.000497  min_lr: 0.000012  loss: 3.9088 (3.9517)  class_acc: 0.6375 (0.6367)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 17.1950 (17.0060)
[2024-07-17 07:02:48]  Epoch: [6]  [ 0/94]  eta: 0:07:48  lr: 0.000497  min_lr: 0.000012  loss: 3.8358 (3.8358)  class_acc: 0.5781 (0.5781)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 18.7790 (18.7790)  time: 4.9842  data: 4.6079  max mem: 14356
[2024-07-17 07:04:00]  Epoch: [6]  [93/94]  eta: 0:00:00  lr: 0.000496  min_lr: 0.000012  loss: 3.4369 (3.6901)  class_acc: 0.6520 (0.6503)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 17.1839 (16.9483)  time: 0.5913  data: 0.2482  max mem: 14356
Epoch: [6] Total time: 0:01:17 (0.8266 s / it)
Averaged stats: lr: 0.000496  min_lr: 0.000012  loss: 3.4369 (3.6901)  class_acc: 0.6520 (0.6503)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 17.1839 (16.9483)
[2024-07-17 07:04:05]  Epoch: [7]  [ 0/94]  eta: 0:07:04  lr: 0.000495  min_lr: 0.000012  loss: 3.1815 (3.1815)  class_acc: 0.6875 (0.6875)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.7709 (14.7709)  time: 4.5115  data: 4.1449  max mem: 14356
[2024-07-17 07:05:16]  Epoch: [7]  [93/94]  eta: 0:00:00  lr: 0.000494  min_lr: 0.000012  loss: 3.4373 (3.4224)  class_acc: 0.6762 (0.6723)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.1326 (16.7767)  time: 0.7695  data: 0.4276  max mem: 14356
Epoch: [7] Total time: 0:01:16 (0.8093 s / it)
Averaged stats: lr: 0.000494  min_lr: 0.000012  loss: 3.4373 (3.4224)  class_acc: 0.6762 (0.6723)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.1326 (16.7767)
[2024-07-17 07:05:21]  Epoch: [8]  [ 0/94]  eta: 0:08:00  lr: 0.000494  min_lr: 0.000012  loss: 3.6018 (3.6018)  class_acc: 0.6875 (0.6875)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 18.1592 (18.1592)  time: 5.1151  data: 4.7555  max mem: 14356
[2024-07-17 07:06:23]  Epoch: [8]  [93/94]  eta: 0:00:00  lr: 0.000492  min_lr: 0.000012  loss: 3.4841 (3.3490)  class_acc: 0.6723 (0.6728)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 17.1047 (17.0998)  time: 0.6090  data: 0.2626  max mem: 14356
Epoch: [8] Total time: 0:01:07 (0.7177 s / it)
Averaged stats: lr: 0.000492  min_lr: 0.000012  loss: 3.4841 (3.3490)  class_acc: 0.6723 (0.6728)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 17.1047 (17.0998)
[2024-07-17 07:06:30]  Epoch: [9]  [ 0/94]  eta: 0:09:02  lr: 0.000492  min_lr: 0.000012  loss: 3.4466 (3.4466)  class_acc: 0.6406 (0.6406)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.2076 (15.2076)  time: 5.7762  data: 5.4132  max mem: 14356
[2024-07-17 07:07:32]  Epoch: [9]  [93/94]  eta: 0:00:00  lr: 0.000490  min_lr: 0.000012  loss: 3.0390 (3.0894)  class_acc: 0.6949 (0.6939)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 17.2835 (17.2267)  time: 0.5228  data: 0.1777  max mem: 14356
Epoch: [9] Total time: 0:01:08 (0.7325 s / it)
Averaged stats: lr: 0.000490  min_lr: 0.000012  loss: 3.0390 (3.0894)  class_acc: 0.6949 (0.6939)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 17.2835 (17.2267)
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-9.pth
[2024-07-17 07:07:36]  Test:  [ 0/16]  eta: 0:00:49  loss: 21.0444 (21.0444)  acc: 0.8229 (0.8229)  recognition_fmeasure: 0.9566 (0.9566)  time: 3.1144  data: 2.4647  max mem: 14356
[2024-07-17 07:07:43]  Test:  [10/16]  eta: 0:00:05  loss: 20.5344 (20.5344)  acc: 0.7547 (0.7547)  recognition_fmeasure: 0.9007 (0.9007)  time: 0.8706  data: 0.2242  max mem: 14356
[2024-07-17 07:07:46]  Test:  [15/16]  eta: 0:00:00  loss: 20.8028 (20.8028)  acc: 0.7640 (0.7630)  recognition_fmeasure: 0.9112 (0.9083)  time: 0.7747  data: 0.1541  max mem: 14356
Test: Total time: 0:00:12 (0.7962 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.7630 loss 20.8028 Rec_fmeasure 0.9083
Accuracy of the network on the 2941 test images: 0.7630%
Max accuracy: 0.77%
[2024-07-17 07:07:51]  Epoch: [10]  [ 0/94]  eta: 0:08:23  lr: 0.000490  min_lr: 0.000012  loss: 3.3774 (3.3774)  class_acc: 0.6641 (0.6641)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.7028 (13.7028)  time: 5.3531  data: 4.9709  max mem: 14356
[2024-07-17 07:09:02]  Epoch: [10]  [93/94]  eta: 0:00:00  lr: 0.000488  min_lr: 0.000012  loss: 2.9316 (3.0498)  class_acc: 0.6859 (0.6928)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.2991 (16.7274)  time: 0.5621  data: 0.2168  max mem: 14356
Epoch: [10] Total time: 0:01:16 (0.8147 s / it)
Averaged stats: lr: 0.000488  min_lr: 0.000012  loss: 2.9316 (3.0498)  class_acc: 0.6859 (0.6928)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.2991 (16.7274)
[2024-07-17 07:09:08]  Epoch: [11]  [ 0/94]  eta: 0:08:06  lr: 0.000488  min_lr: 0.000012  loss: 3.0101 (3.0101)  class_acc: 0.7031 (0.7031)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.4670 (16.4670)  time: 5.1750  data: 4.8119  max mem: 14356
[2024-07-17 07:10:18]  Epoch: [11]  [93/94]  eta: 0:00:00  lr: 0.000485  min_lr: 0.000012  loss: 2.9043 (2.9033)  class_acc: 0.7020 (0.7042)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.9778 (16.6323)  time: 0.6189  data: 0.2716  max mem: 14356
Epoch: [11] Total time: 0:01:15 (0.8061 s / it)
Averaged stats: lr: 0.000485  min_lr: 0.000012  loss: 2.9043 (2.9033)  class_acc: 0.7020 (0.7042)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.9778 (16.6323)
[2024-07-17 07:10:22]  Epoch: [12]  [ 0/94]  eta: 0:05:28  lr: 0.000485  min_lr: 0.000012  loss: 2.6883 (2.6883)  class_acc: 0.7500 (0.7500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.9636 (12.9636)  time: 3.4950  data: 3.1313  max mem: 14356
[2024-07-17 07:11:33]  Epoch: [12]  [93/94]  eta: 0:00:00  lr: 0.000482  min_lr: 0.000011  loss: 2.7956 (2.7899)  class_acc: 0.7219 (0.7172)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.2351 (16.1151)  time: 0.8533  data: 0.5078  max mem: 14356
Epoch: [12] Total time: 0:01:15 (0.7999 s / it)
Averaged stats: lr: 0.000482  min_lr: 0.000011  loss: 2.7956 (2.7899)  class_acc: 0.7219 (0.7172)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.2351 (16.1151)
[2024-07-17 07:11:40]  Epoch: [13]  [ 0/94]  eta: 0:09:55  lr: 0.000482  min_lr: 0.000011  loss: 2.6732 (2.6732)  class_acc: 0.7188 (0.7188)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.4993 (13.4993)  time: 6.3335  data: 5.9714  max mem: 14356
[2024-07-17 07:12:43]  Epoch: [13]  [93/94]  eta: 0:00:00  lr: 0.000479  min_lr: 0.000011  loss: 2.7065 (2.7063)  class_acc: 0.7203 (0.7227)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.2750 (16.5293)  time: 0.7345  data: 0.3832  max mem: 14356
Epoch: [13] Total time: 0:01:09 (0.7430 s / it)
Averaged stats: lr: 0.000479  min_lr: 0.000011  loss: 2.7065 (2.7063)  class_acc: 0.7203 (0.7227)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.2750 (16.5293)
[2024-07-17 07:12:50]  Epoch: [14]  [ 0/94]  eta: 0:10:12  lr: 0.000479  min_lr: 0.000011  loss: 3.7081 (3.7081)  class_acc: 0.7109 (0.7109)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.1694 (15.1694)  time: 6.5155  data: 6.1558  max mem: 14356
[2024-07-17 07:13:54]  Epoch: [14]  [93/94]  eta: 0:00:00  lr: 0.000476  min_lr: 0.000011  loss: 2.4927 (2.5430)  class_acc: 0.7309 (0.7304)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.9066 (16.4131)  time: 0.7489  data: 0.4012  max mem: 14356
Epoch: [14] Total time: 0:01:11 (0.7565 s / it)
Averaged stats: lr: 0.000476  min_lr: 0.000011  loss: 2.4927 (2.5430)  class_acc: 0.7309 (0.7304)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.9066 (16.4131)
[2024-07-17 07:13:58]  Test:  [ 0/16]  eta: 0:01:02  loss: 20.9661 (20.9661)  acc: 0.8490 (0.8490)  recognition_fmeasure: 0.9670 (0.9670)  time: 3.8898  data: 3.2390  max mem: 14356
[2024-07-17 07:14:05]  Test:  [10/16]  eta: 0:00:05  loss: 20.3546 (20.3546)  acc: 0.7860 (0.7860)  recognition_fmeasure: 0.9036 (0.9036)  time: 0.9400  data: 0.2946  max mem: 14356
[2024-07-17 07:14:08]  Test:  [15/16]  eta: 0:00:00  loss: 20.6025 (20.6025)  acc: 0.7957 (0.7939)  recognition_fmeasure: 0.9130 (0.9102)  time: 0.8260  data: 0.2025  max mem: 14356
Test: Total time: 0:00:13 (0.8491 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.7939 loss 20.6025 Rec_fmeasure 0.9102
Accuracy of the network on the 2941 test images: 0.7939%
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-best.pth
Max accuracy: 0.79%
[2024-07-17 07:14:16]  Epoch: [15]  [ 0/94]  eta: 0:09:36  lr: 0.000476  min_lr: 0.000011  loss: 2.9847 (2.9847)  class_acc: 0.7422 (0.7422)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.7429 (15.7429)  time: 6.1366  data: 5.7752  max mem: 14356
[2024-07-17 07:15:16]  Epoch: [15]  [93/94]  eta: 0:00:00  lr: 0.000472  min_lr: 0.000011  loss: 2.3796 (2.4424)  class_acc: 0.7430 (0.7385)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.3091 (16.5412)  time: 0.6293  data: 0.2847  max mem: 14356
Epoch: [15] Total time: 0:01:07 (0.7144 s / it)
Averaged stats: lr: 0.000472  min_lr: 0.000011  loss: 2.3796 (2.4424)  class_acc: 0.7430 (0.7385)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.3091 (16.5412)
[2024-07-17 07:15:22]  Epoch: [16]  [ 0/94]  eta: 0:07:57  lr: 0.000472  min_lr: 0.000011  loss: 2.1544 (2.1544)  class_acc: 0.7656 (0.7656)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.0605 (15.0605)  time: 5.0843  data: 4.7175  max mem: 14356
[2024-07-17 07:16:35]  Epoch: [16]  [93/94]  eta: 0:00:00  lr: 0.000469  min_lr: 0.000011  loss: 2.2919 (2.3502)  class_acc: 0.7461 (0.7434)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.0212 (16.0565)  time: 0.7951  data: 0.4514  max mem: 14356
Epoch: [16] Total time: 0:01:18 (0.8322 s / it)
Averaged stats: lr: 0.000469  min_lr: 0.000011  loss: 2.2919 (2.3502)  class_acc: 0.7461 (0.7434)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.0212 (16.0565)
[2024-07-17 07:16:41]  Epoch: [17]  [ 0/94]  eta: 0:08:47  lr: 0.000469  min_lr: 0.000011  loss: 2.4317 (2.4317)  class_acc: 0.7422 (0.7422)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 17.6847 (17.6847)  time: 5.6101  data: 5.2480  max mem: 14356
[2024-07-17 07:17:49]  Epoch: [17]  [93/94]  eta: 0:00:00  lr: 0.000465  min_lr: 0.000011  loss: 2.3501 (2.3040)  class_acc: 0.7566 (0.7503)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.4798 (16.3674)  time: 0.8840  data: 0.5401  max mem: 14356
Epoch: [17] Total time: 0:01:14 (0.7888 s / it)
Averaged stats: lr: 0.000465  min_lr: 0.000011  loss: 2.3501 (2.3040)  class_acc: 0.7566 (0.7503)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.4798 (16.3674)
[2024-07-17 07:17:55]  Epoch: [18]  [ 0/94]  eta: 0:08:58  lr: 0.000465  min_lr: 0.000011  loss: 2.2128 (2.2128)  class_acc: 0.7422 (0.7422)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.0448 (16.0448)  time: 5.7339  data: 5.3752  max mem: 14356
[2024-07-17 07:18:58]  Epoch: [18]  [93/94]  eta: 0:00:00  lr: 0.000460  min_lr: 0.000011  loss: 2.1816 (2.2710)  class_acc: 0.7520 (0.7524)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.8961 (16.2481)  time: 0.7054  data: 0.3570  max mem: 14356
Epoch: [18] Total time: 0:01:08 (0.7331 s / it)
Averaged stats: lr: 0.000460  min_lr: 0.000011  loss: 2.1816 (2.2710)  class_acc: 0.7520 (0.7524)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.8961 (16.2481)
[2024-07-17 07:19:03]  Epoch: [19]  [ 0/94]  eta: 0:08:21  lr: 0.000460  min_lr: 0.000011  loss: 1.9842 (1.9842)  class_acc: 0.7812 (0.7812)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.4972 (11.4972)  time: 5.3387  data: 4.9806  max mem: 14356
[2024-07-17 07:20:09]  Epoch: [19]  [93/94]  eta: 0:00:00  lr: 0.000456  min_lr: 0.000011  loss: 2.2509 (2.2069)  class_acc: 0.7434 (0.7556)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 17.3230 (16.6068)  time: 0.5795  data: 0.2337  max mem: 14356
Epoch: [19] Total time: 0:01:10 (0.7552 s / it)
Averaged stats: lr: 0.000456  min_lr: 0.000011  loss: 2.2509 (2.2069)  class_acc: 0.7434 (0.7556)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 17.3230 (16.6068)
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-19.pth
[2024-07-17 07:20:14]  Test:  [ 0/16]  eta: 0:01:10  loss: 20.8552 (20.8552)  acc: 0.8542 (0.8542)  recognition_fmeasure: 0.9685 (0.9685)  time: 4.4140  data: 3.7600  max mem: 14356
[2024-07-17 07:20:20]  Test:  [10/16]  eta: 0:00:05  loss: 20.3199 (20.3199)  acc: 0.7893 (0.7893)  recognition_fmeasure: 0.9069 (0.9069)  time: 0.9923  data: 0.3419  max mem: 14356
[2024-07-17 07:20:23]  Test:  [15/16]  eta: 0:00:00  loss: 20.5611 (20.5611)  acc: 0.8017 (0.7987)  recognition_fmeasure: 0.9161 (0.9131)  time: 0.8585  data: 0.2351  max mem: 14356
Test: Total time: 0:00:14 (0.8814 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.7987 loss 20.5611 Rec_fmeasure 0.9131
Accuracy of the network on the 2941 test images: 0.7987%
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-best.pth
Max accuracy: 0.80%
[2024-07-17 07:20:32]  Epoch: [20]  [ 0/94]  eta: 0:10:20  lr: 0.000456  min_lr: 0.000011  loss: 1.8269 (1.8269)  class_acc: 0.7578 (0.7578)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.8409 (13.8409)  time: 6.5993  data: 6.2332  max mem: 14356
[2024-07-17 07:21:35]  Epoch: [20]  [93/94]  eta: 0:00:00  lr: 0.000451  min_lr: 0.000011  loss: 2.0740 (2.1048)  class_acc: 0.7539 (0.7542)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.9496 (16.3818)  time: 0.5693  data: 0.2216  max mem: 14356
Epoch: [20] Total time: 0:01:10 (0.7531 s / it)
Averaged stats: lr: 0.000451  min_lr: 0.000011  loss: 2.0740 (2.1048)  class_acc: 0.7539 (0.7542)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.9496 (16.3818)
[2024-07-17 07:21:41]  Epoch: [21]  [ 0/94]  eta: 0:08:08  lr: 0.000451  min_lr: 0.000011  loss: 1.7499 (1.7499)  class_acc: 0.7578 (0.7578)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.0768 (14.0768)  time: 5.1979  data: 4.8332  max mem: 14356
[2024-07-17 07:22:55]  Epoch: [21]  [93/94]  eta: 0:00:00  lr: 0.000447  min_lr: 0.000011  loss: 2.0204 (2.1247)  class_acc: 0.7699 (0.7656)  loss_scale: 32768.0000 (44968.8511)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.8067 (inf)  time: 0.5534  data: 0.2100  max mem: 14356
Epoch: [21] Total time: 0:01:19 (0.8451 s / it)
Averaged stats: lr: 0.000447  min_lr: 0.000011  loss: 2.0204 (2.1247)  class_acc: 0.7699 (0.7656)  loss_scale: 32768.0000 (44968.8511)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.8067 (inf)
[2024-07-17 07:23:00]  Epoch: [22]  [ 0/94]  eta: 0:06:35  lr: 0.000447  min_lr: 0.000011  loss: 1.8680 (1.8680)  class_acc: 0.7500 (0.7500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.6126 (14.6126)  time: 4.2041  data: 3.8476  max mem: 14356
[2024-07-17 07:24:03]  Epoch: [22]  [93/94]  eta: 0:00:00  lr: 0.000442  min_lr: 0.000010  loss: 1.8982 (1.9358)  class_acc: 0.7770 (0.7741)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.0218 (15.2744)  time: 0.7675  data: 0.4238  max mem: 14356
Epoch: [22] Total time: 0:01:07 (0.7229 s / it)
Averaged stats: lr: 0.000442  min_lr: 0.000010  loss: 1.8982 (1.9358)  class_acc: 0.7770 (0.7741)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.0218 (15.2744)
[2024-07-17 07:24:09]  Epoch: [23]  [ 0/94]  eta: 0:09:00  lr: 0.000442  min_lr: 0.000010  loss: 1.7991 (1.7991)  class_acc: 0.7500 (0.7500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.9035 (13.9035)  time: 5.7469  data: 5.3818  max mem: 14356
[2024-07-17 07:25:08]  Epoch: [23]  [93/94]  eta: 0:00:00  lr: 0.000436  min_lr: 0.000010  loss: 1.8790 (1.9056)  class_acc: 0.7758 (0.7815)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.2252 (15.7786)  time: 0.4287  data: 0.0844  max mem: 14356
Epoch: [23] Total time: 0:01:05 (0.6935 s / it)
Averaged stats: lr: 0.000436  min_lr: 0.000010  loss: 1.8790 (1.9056)  class_acc: 0.7758 (0.7815)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.2252 (15.7786)
[2024-07-17 07:25:15]  Epoch: [24]  [ 0/94]  eta: 0:10:31  lr: 0.000436  min_lr: 0.000010  loss: 2.4349 (2.4349)  class_acc: 0.7266 (0.7266)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 22.2569 (22.2569)  time: 6.7185  data: 6.3555  max mem: 14356
[2024-07-17 07:26:25]  Epoch: [24]  [93/94]  eta: 0:00:00  lr: 0.000431  min_lr: 0.000010  loss: 1.7667 (1.8392)  class_acc: 0.7703 (0.7797)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.1702 (15.6180)  time: 0.4769  data: 0.1296  max mem: 14356
Epoch: [24] Total time: 0:01:16 (0.8124 s / it)
Averaged stats: lr: 0.000431  min_lr: 0.000010  loss: 1.7667 (1.8392)  class_acc: 0.7703 (0.7797)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.1702 (15.6180)
[2024-07-17 07:26:28]  Test:  [ 0/16]  eta: 0:00:49  loss: 21.0174 (21.0174)  acc: 0.8333 (0.8333)  recognition_fmeasure: 0.9530 (0.9530)  time: 3.1093  data: 2.4562  max mem: 14356
[2024-07-17 07:26:34]  Test:  [10/16]  eta: 0:00:05  loss: 20.2956 (20.2956)  acc: 0.7902 (0.7902)  recognition_fmeasure: 0.9060 (0.9060)  time: 0.8731  data: 0.2234  max mem: 14356
[2024-07-17 07:26:37]  Test:  [15/16]  eta: 0:00:00  loss: 20.5582 (20.5582)  acc: 0.7971 (0.7946)  recognition_fmeasure: 0.9143 (0.9116)  time: 0.7764  data: 0.1536  max mem: 14356
Test: Total time: 0:00:12 (0.7988 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.7946 loss 20.5582 Rec_fmeasure 0.9116
Accuracy of the network on the 2941 test images: 0.7946%
Max accuracy: 0.80%
[2024-07-17 07:26:43]  Epoch: [25]  [ 0/94]  eta: 0:08:52  lr: 0.000431  min_lr: 0.000010  loss: 1.3138 (1.3138)  class_acc: 0.8125 (0.8125)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.8620 (12.8620)  time: 5.6666  data: 5.3047  max mem: 14356
[2024-07-17 07:27:54]  Epoch: [25]  [93/94]  eta: 0:00:00  lr: 0.000426  min_lr: 0.000010  loss: 1.7753 (1.8115)  class_acc: 0.7777 (0.7827)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.1248 (15.6578)  time: 0.6008  data: 0.2513  max mem: 14356
Epoch: [25] Total time: 0:01:16 (0.8150 s / it)
Averaged stats: lr: 0.000426  min_lr: 0.000010  loss: 1.7753 (1.8115)  class_acc: 0.7777 (0.7827)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.1248 (15.6578)
[2024-07-17 07:27:59]  Epoch: [26]  [ 0/94]  eta: 0:07:02  lr: 0.000426  min_lr: 0.000010  loss: 1.5496 (1.5496)  class_acc: 0.7656 (0.7656)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.2982 (16.2982)  time: 4.4920  data: 4.1279  max mem: 14356
[2024-07-17 07:29:12]  Epoch: [26]  [93/94]  eta: 0:00:00  lr: 0.000420  min_lr: 0.000010  loss: 1.7293 (1.7064)  class_acc: 0.7883 (0.7916)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.5951 (15.2308)  time: 0.7579  data: 0.4142  max mem: 14356
Epoch: [26] Total time: 0:01:17 (0.8295 s / it)
Averaged stats: lr: 0.000420  min_lr: 0.000010  loss: 1.7293 (1.7064)  class_acc: 0.7883 (0.7916)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.5951 (15.2308)
[2024-07-17 07:29:17]  Epoch: [27]  [ 0/94]  eta: 0:07:04  lr: 0.000420  min_lr: 0.000010  loss: 1.5304 (1.5304)  class_acc: 0.7734 (0.7734)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.9602 (14.9602)  time: 4.5210  data: 4.1572  max mem: 14356
[2024-07-17 07:30:28]  Epoch: [27]  [93/94]  eta: 0:00:00  lr: 0.000414  min_lr: 0.000010  loss: 1.6788 (1.7354)  class_acc: 0.7906 (0.7940)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.0382 (15.5829)  time: 0.8243  data: 0.4767  max mem: 14356
Epoch: [27] Total time: 0:01:16 (0.8123 s / it)
Averaged stats: lr: 0.000414  min_lr: 0.000010  loss: 1.6788 (1.7354)  class_acc: 0.7906 (0.7940)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.0382 (15.5829)
[2024-07-17 07:30:36]  Epoch: [28]  [ 0/94]  eta: 0:11:11  lr: 0.000414  min_lr: 0.000010  loss: 1.3900 (1.3900)  class_acc: 0.7891 (0.7891)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.7178 (15.7178)  time: 7.1397  data: 6.7807  max mem: 14356
[2024-07-17 07:31:26]  Epoch: [28]  [93/94]  eta: 0:00:00  lr: 0.000408  min_lr: 0.000010  loss: 1.6747 (1.6457)  class_acc: 0.7973 (0.7990)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.7546 (15.0952)  time: 0.3446  data: 0.0001  max mem: 14356
Epoch: [28] Total time: 0:00:58 (0.6197 s / it)
Averaged stats: lr: 0.000408  min_lr: 0.000010  loss: 1.6747 (1.6457)  class_acc: 0.7973 (0.7990)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.7546 (15.0952)
[2024-07-17 07:31:32]  Epoch: [29]  [ 0/94]  eta: 0:08:28  lr: 0.000408  min_lr: 0.000010  loss: 1.4241 (1.4241)  class_acc: 0.7500 (0.7500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 18.6431 (18.6431)  time: 5.4124  data: 5.0514  max mem: 14356
[2024-07-17 07:32:45]  Epoch: [29]  [93/94]  eta: 0:00:00  lr: 0.000402  min_lr: 0.000010  loss: 1.5384 (1.5458)  class_acc: 0.8102 (0.8073)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.3169 (14.7275)  time: 0.5832  data: 0.2325  max mem: 14356
Epoch: [29] Total time: 0:01:18 (0.8358 s / it)
Averaged stats: lr: 0.000402  min_lr: 0.000010  loss: 1.5384 (1.5458)  class_acc: 0.8102 (0.8073)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.3169 (14.7275)
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-29.pth
[2024-07-17 07:32:50]  Test:  [ 0/16]  eta: 0:01:01  loss: 20.8059 (20.8059)  acc: 0.8490 (0.8490)  recognition_fmeasure: 0.9592 (0.9592)  time: 3.8192  data: 3.1719  max mem: 14356
[2024-07-17 07:32:56]  Test:  [10/16]  eta: 0:00:05  loss: 20.2732 (20.2732)  acc: 0.8002 (0.8002)  recognition_fmeasure: 0.9049 (0.9049)  time: 0.9338  data: 0.2884  max mem: 14356
[2024-07-17 07:32:59]  Test:  [15/16]  eta: 0:00:00  loss: 20.5241 (20.5241)  acc: 0.8082 (0.8062)  recognition_fmeasure: 0.9135 (0.9109)  time: 0.8183  data: 0.1983  max mem: 14356
Test: Total time: 0:00:13 (0.8398 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.8062 loss 20.5241 Rec_fmeasure 0.9109
Accuracy of the network on the 2941 test images: 0.8062%
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-best.pth
Max accuracy: 0.81%
[2024-07-17 07:33:06]  Epoch: [30]  [ 0/94]  eta: 0:08:12  lr: 0.000402  min_lr: 0.000010  loss: 1.9620 (1.9620)  class_acc: 0.8047 (0.8047)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.8243 (14.8243)  time: 5.2379  data: 4.8595  max mem: 14356
[2024-07-17 07:34:12]  Epoch: [30]  [93/94]  eta: 0:00:00  lr: 0.000395  min_lr: 0.000009  loss: 1.4550 (1.5084)  class_acc: 0.8137 (0.8086)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.9210 (15.4724)  time: 0.5698  data: 0.2257  max mem: 14356
Epoch: [30] Total time: 0:01:10 (0.7552 s / it)
Averaged stats: lr: 0.000395  min_lr: 0.000009  loss: 1.4550 (1.5084)  class_acc: 0.8137 (0.8086)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.9210 (15.4724)
[2024-07-17 07:34:17]  Epoch: [31]  [ 0/94]  eta: 0:07:54  lr: 0.000395  min_lr: 0.000009  loss: 1.8214 (1.8214)  class_acc: 0.7969 (0.7969)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.9229 (16.9229)  time: 5.0465  data: 4.6861  max mem: 14356
[2024-07-17 07:35:31]  Epoch: [31]  [93/94]  eta: 0:00:00  lr: 0.000389  min_lr: 0.000009  loss: 1.4609 (1.5203)  class_acc: 0.8203 (0.8107)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.7831 (15.1834)  time: 0.7827  data: 0.4388  max mem: 14356
Epoch: [31] Total time: 0:01:19 (0.8420 s / it)
Averaged stats: lr: 0.000389  min_lr: 0.000009  loss: 1.4609 (1.5203)  class_acc: 0.8203 (0.8107)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.7831 (15.1834)
[2024-07-17 07:35:36]  Epoch: [32]  [ 0/94]  eta: 0:08:12  lr: 0.000389  min_lr: 0.000009  loss: 1.6597 (1.6597)  class_acc: 0.7812 (0.7812)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 18.5299 (18.5299)  time: 5.2423  data: 4.8769  max mem: 14356
[2024-07-17 07:36:48]  Epoch: [32]  [93/94]  eta: 0:00:00  lr: 0.000382  min_lr: 0.000009  loss: 1.4581 (1.4954)  class_acc: 0.8195 (0.8153)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.7919 (14.8164)  time: 0.8282  data: 0.4785  max mem: 14356
Epoch: [32] Total time: 0:01:16 (0.8185 s / it)
Averaged stats: lr: 0.000382  min_lr: 0.000009  loss: 1.4581 (1.4954)  class_acc: 0.8195 (0.8153)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.7919 (14.8164)
[2024-07-17 07:36:53]  Epoch: [33]  [ 0/94]  eta: 0:07:59  lr: 0.000382  min_lr: 0.000009  loss: 1.4048 (1.4048)  class_acc: 0.8281 (0.8281)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.0273 (13.0273)  time: 5.1031  data: 4.7329  max mem: 14356
[2024-07-17 07:37:56]  Epoch: [33]  [93/94]  eta: 0:00:00  lr: 0.000375  min_lr: 0.000009  loss: 1.3711 (1.4060)  class_acc: 0.8207 (0.8143)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.5968 (15.0233)  time: 0.5761  data: 0.2335  max mem: 14356
Epoch: [33] Total time: 0:01:08 (0.7313 s / it)
Averaged stats: lr: 0.000375  min_lr: 0.000009  loss: 1.3711 (1.4060)  class_acc: 0.8207 (0.8143)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.5968 (15.0233)
[2024-07-17 07:38:02]  Epoch: [34]  [ 0/94]  eta: 0:08:03  lr: 0.000375  min_lr: 0.000009  loss: 1.4507 (1.4507)  class_acc: 0.8281 (0.8281)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.5949 (12.5949)  time: 5.1480  data: 4.7877  max mem: 14356
[2024-07-17 07:39:05]  Epoch: [34]  [93/94]  eta: 0:00:00  lr: 0.000368  min_lr: 0.000009  loss: 1.3739 (1.4033)  class_acc: 0.8324 (0.8229)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.0557 (15.5363)  time: 0.5486  data: 0.2037  max mem: 14356
Epoch: [34] Total time: 0:01:08 (0.7291 s / it)
Averaged stats: lr: 0.000368  min_lr: 0.000009  loss: 1.3739 (1.4033)  class_acc: 0.8324 (0.8229)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.0557 (15.5363)
[2024-07-17 07:39:08]  Test:  [ 0/16]  eta: 0:00:49  loss: 20.8074 (20.8074)  acc: 0.8750 (0.8750)  recognition_fmeasure: 0.9605 (0.9605)  time: 3.1137  data: 2.4611  max mem: 14356
[2024-07-17 07:39:15]  Test:  [10/16]  eta: 0:00:05  loss: 20.2607 (20.2607)  acc: 0.7955 (0.7955)  recognition_fmeasure: 0.9116 (0.9116)  time: 0.8702  data: 0.2238  max mem: 14356
[2024-07-17 07:39:18]  Test:  [15/16]  eta: 0:00:00  loss: 20.5090 (20.5090)  acc: 0.8033 (0.8004)  recognition_fmeasure: 0.9186 (0.9159)  time: 0.7782  data: 0.1539  max mem: 14356
Test: Total time: 0:00:12 (0.8022 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.8004 loss 20.5090 Rec_fmeasure 0.9159
Accuracy of the network on the 2941 test images: 0.8004%
Max accuracy: 0.81%
[2024-07-17 07:39:23]  Epoch: [35]  [ 0/94]  eta: 0:08:03  lr: 0.000368  min_lr: 0.000009  loss: 1.3663 (1.3663)  class_acc: 0.8047 (0.8047)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.1142 (14.1142)  time: 5.1489  data: 4.7883  max mem: 14356
[2024-07-17 07:40:31]  Epoch: [35]  [93/94]  eta: 0:00:00  lr: 0.000361  min_lr: 0.000009  loss: 1.3411 (1.3269)  class_acc: 0.8289 (0.8273)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.0034 (14.5296)  time: 0.6119  data: 0.2593  max mem: 14356
Epoch: [35] Total time: 0:01:13 (0.7812 s / it)
Averaged stats: lr: 0.000361  min_lr: 0.000009  loss: 1.3411 (1.3269)  class_acc: 0.8289 (0.8273)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.0034 (14.5296)
[2024-07-17 07:40:36]  Epoch: [36]  [ 0/94]  eta: 0:06:30  lr: 0.000361  min_lr: 0.000009  loss: 1.6127 (1.6127)  class_acc: 0.8281 (0.8281)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.2581 (15.2581)  time: 4.1528  data: 3.7931  max mem: 14356
[2024-07-17 07:41:48]  Epoch: [36]  [93/94]  eta: 0:00:00  lr: 0.000354  min_lr: 0.000008  loss: 1.1898 (1.2966)  class_acc: 0.8336 (0.8292)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.3576 (14.4265)  time: 0.7467  data: 0.4046  max mem: 14356
Epoch: [36] Total time: 0:01:17 (0.8193 s / it)
Averaged stats: lr: 0.000354  min_lr: 0.000008  loss: 1.1898 (1.2966)  class_acc: 0.8336 (0.8292)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.3576 (14.4265)
[2024-07-17 07:41:53]  Epoch: [37]  [ 0/94]  eta: 0:06:33  lr: 0.000354  min_lr: 0.000008  loss: 1.4039 (1.4039)  class_acc: 0.8203 (0.8203)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.1121 (13.1121)  time: 4.1845  data: 3.8204  max mem: 14356
[2024-07-17 07:42:58]  Epoch: [37]  [93/94]  eta: 0:00:00  lr: 0.000347  min_lr: 0.000008  loss: 1.2161 (1.2753)  class_acc: 0.8352 (0.8312)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.3128 (14.7922)  time: 0.7203  data: 0.3760  max mem: 14356
Epoch: [37] Total time: 0:01:09 (0.7410 s / it)
Averaged stats: lr: 0.000347  min_lr: 0.000008  loss: 1.2161 (1.2753)  class_acc: 0.8352 (0.8312)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.3128 (14.7922)
[2024-07-17 07:43:04]  Epoch: [38]  [ 0/94]  eta: 0:08:45  lr: 0.000347  min_lr: 0.000008  loss: 1.6378 (1.6378)  class_acc: 0.8359 (0.8359)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.5091 (13.5091)  time: 5.5873  data: 5.2303  max mem: 14356
[2024-07-17 07:44:06]  Epoch: [38]  [93/94]  eta: 0:00:00  lr: 0.000340  min_lr: 0.000008  loss: 1.1904 (1.1840)  class_acc: 0.8473 (0.8430)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.5611 (14.6084)  time: 0.6424  data: 0.2972  max mem: 14356
Epoch: [38] Total time: 0:01:08 (0.7245 s / it)
Averaged stats: lr: 0.000340  min_lr: 0.000008  loss: 1.1904 (1.1840)  class_acc: 0.8473 (0.8430)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.5611 (14.6084)
[2024-07-17 07:44:11]  Epoch: [39]  [ 0/94]  eta: 0:08:02  lr: 0.000340  min_lr: 0.000008  loss: 1.3300 (1.3300)  class_acc: 0.8281 (0.8281)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.2460 (13.2460)  time: 5.1288  data: 4.7643  max mem: 14356
[2024-07-17 07:45:18]  Epoch: [39]  [93/94]  eta: 0:00:00  lr: 0.000332  min_lr: 0.000008  loss: 1.2386 (1.2507)  class_acc: 0.8355 (0.8358)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.5932 (14.8686)  time: 0.6509  data: 0.3063  max mem: 14356
Epoch: [39] Total time: 0:01:12 (0.7712 s / it)
Averaged stats: lr: 0.000332  min_lr: 0.000008  loss: 1.2386 (1.2507)  class_acc: 0.8355 (0.8358)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.5932 (14.8686)
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-39.pth
[2024-07-17 07:45:23]  Test:  [ 0/16]  eta: 0:00:59  loss: 20.8428 (20.8428)  acc: 0.8698 (0.8698)  recognition_fmeasure: 0.9624 (0.9624)  time: 3.7426  data: 3.0863  max mem: 14356
[2024-07-17 07:45:30]  Test:  [10/16]  eta: 0:00:05  loss: 20.2764 (20.2764)  acc: 0.7964 (0.7964)  recognition_fmeasure: 0.9106 (0.9106)  time: 0.9272  data: 0.2807  max mem: 14356
[2024-07-17 07:45:32]  Test:  [15/16]  eta: 0:00:00  loss: 20.5230 (20.5230)  acc: 0.8072 (0.8052)  recognition_fmeasure: 0.9200 (0.9173)  time: 0.8136  data: 0.1930  max mem: 14356
Test: Total time: 0:00:13 (0.8366 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.8052 loss 20.5230 Rec_fmeasure 0.9173
Accuracy of the network on the 2941 test images: 0.8052%
Max accuracy: 0.81%
[2024-07-17 07:45:39]  Epoch: [40]  [ 0/94]  eta: 0:09:40  lr: 0.000332  min_lr: 0.000008  loss: 1.3027 (1.3027)  class_acc: 0.8125 (0.8125)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.8359 (11.8359)  time: 6.1777  data: 5.8214  max mem: 14356
[2024-07-17 07:46:43]  Epoch: [40]  [93/94]  eta: 0:00:00  lr: 0.000325  min_lr: 0.000008  loss: 1.0773 (1.1655)  class_acc: 0.8512 (0.8469)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.1040 (14.4862)  time: 0.4862  data: 0.1398  max mem: 14356
Epoch: [40] Total time: 0:01:10 (0.7519 s / it)
Averaged stats: lr: 0.000325  min_lr: 0.000008  loss: 1.0773 (1.1655)  class_acc: 0.8512 (0.8469)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.1040 (14.4862)
[2024-07-17 07:46:48]  Epoch: [41]  [ 0/94]  eta: 0:07:34  lr: 0.000325  min_lr: 0.000008  loss: 1.0101 (1.0101)  class_acc: 0.8359 (0.8359)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.2391 (14.2391)  time: 4.8333  data: 4.4724  max mem: 14356
[2024-07-17 07:48:00]  Epoch: [41]  [93/94]  eta: 0:00:00  lr: 0.000317  min_lr: 0.000008  loss: 1.1392 (1.1104)  class_acc: 0.8449 (0.8491)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.8978 (14.2933)  time: 0.7015  data: 0.3560  max mem: 14356
Epoch: [41] Total time: 0:01:17 (0.8195 s / it)
Averaged stats: lr: 0.000317  min_lr: 0.000008  loss: 1.1392 (1.1104)  class_acc: 0.8449 (0.8491)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.8978 (14.2933)
[2024-07-17 07:48:07]  Epoch: [42]  [ 0/94]  eta: 0:10:29  lr: 0.000317  min_lr: 0.000008  loss: 1.3142 (1.3142)  class_acc: 0.8281 (0.8281)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.5128 (12.5128)  time: 6.6997  data: 6.3356  max mem: 14356
[2024-07-17 07:49:13]  Epoch: [42]  [93/94]  eta: 0:00:00  lr: 0.000309  min_lr: 0.000007  loss: 1.1417 (1.1240)  class_acc: 0.8457 (0.8487)  loss_scale: 44236.8008 (35208.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.8483 (14.1777)  time: 0.7519  data: 0.4086  max mem: 14356
Epoch: [42] Total time: 0:01:12 (0.7740 s / it)
Averaged stats: lr: 0.000309  min_lr: 0.000007  loss: 1.1417 (1.1240)  class_acc: 0.8457 (0.8487)  loss_scale: 44236.8008 (35208.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.8483 (14.1777)
[2024-07-17 07:49:19]  Epoch: [43]  [ 0/94]  eta: 0:09:20  lr: 0.000309  min_lr: 0.000007  loss: 0.8469 (0.8469)  class_acc: 0.8828 (0.8828)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.2886 (14.2886)  time: 5.9670  data: 5.6041  max mem: 14356
[2024-07-17 07:50:23]  Epoch: [43]  [93/94]  eta: 0:00:00  lr: 0.000302  min_lr: 0.000007  loss: 1.0396 (1.0716)  class_acc: 0.8465 (0.8500)  loss_scale: 32768.0000 (34510.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.8546 (nan)  time: 0.6460  data: 0.3003  max mem: 14356
Epoch: [43] Total time: 0:01:10 (0.7505 s / it)
Averaged stats: lr: 0.000302  min_lr: 0.000007  loss: 1.0396 (1.0716)  class_acc: 0.8465 (0.8500)  loss_scale: 32768.0000 (34510.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.8546 (nan)
[2024-07-17 07:50:31]  Epoch: [44]  [ 0/94]  eta: 0:10:48  lr: 0.000302  min_lr: 0.000007  loss: 1.5287 (1.5287)  class_acc: 0.8672 (0.8672)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.2065 (15.2065)  time: 6.8998  data: 6.5365  max mem: 14356
[2024-07-17 07:51:37]  Epoch: [44]  [93/94]  eta: 0:00:00  lr: 0.000294  min_lr: 0.000007  loss: 1.0037 (0.9998)  class_acc: 0.8512 (0.8622)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.3961 (13.7333)  time: 0.5448  data: 0.2043  max mem: 14356
Epoch: [44] Total time: 0:01:13 (0.7854 s / it)
Averaged stats: lr: 0.000294  min_lr: 0.000007  loss: 1.0037 (0.9998)  class_acc: 0.8512 (0.8622)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.3961 (13.7333)
[2024-07-17 07:51:41]  Test:  [ 0/16]  eta: 0:00:52  loss: 20.8383 (20.8383)  acc: 0.8594 (0.8594)  recognition_fmeasure: 0.9555 (0.9555)  time: 3.2666  data: 2.6169  max mem: 14356
[2024-07-17 07:51:47]  Test:  [10/16]  eta: 0:00:05  loss: 20.2420 (20.2420)  acc: 0.8011 (0.8011)  recognition_fmeasure: 0.9084 (0.9084)  time: 0.8838  data: 0.2380  max mem: 14356
[2024-07-17 07:51:50]  Test:  [15/16]  eta: 0:00:00  loss: 20.4912 (20.4912)  acc: 0.8107 (0.8096)  recognition_fmeasure: 0.9176 (0.9153)  time: 0.7838  data: 0.1636  max mem: 14356
Test: Total time: 0:00:12 (0.8070 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.8096 loss 20.4912 Rec_fmeasure 0.9153
Accuracy of the network on the 2941 test images: 0.8096%
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-best.pth
Max accuracy: 0.81%
[2024-07-17 07:51:59]  Epoch: [45]  [ 0/94]  eta: 0:10:33  lr: 0.000294  min_lr: 0.000007  loss: 1.4063 (1.4063)  class_acc: 0.8203 (0.8203)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.5663 (13.5663)  time: 6.7446  data: 6.3814  max mem: 14356
[2024-07-17 07:53:03]  Epoch: [45]  [93/94]  eta: 0:00:00  lr: 0.000286  min_lr: 0.000007  loss: 0.8878 (0.9321)  class_acc: 0.8758 (0.8674)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.0614 (13.3291)  time: 0.4904  data: 0.1486  max mem: 14356
Epoch: [45] Total time: 0:01:11 (0.7597 s / it)
Averaged stats: lr: 0.000286  min_lr: 0.000007  loss: 0.8878 (0.9321)  class_acc: 0.8758 (0.8674)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.0614 (13.3291)
[2024-07-17 07:53:10]  Epoch: [46]  [ 0/94]  eta: 0:10:37  lr: 0.000286  min_lr: 0.000007  loss: 1.1080 (1.1080)  class_acc: 0.8750 (0.8750)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.8300 (11.8300)  time: 6.7817  data: 6.4195  max mem: 14356
[2024-07-17 07:54:21]  Epoch: [46]  [93/94]  eta: 0:00:00  lr: 0.000278  min_lr: 0.000007  loss: 0.8336 (0.9474)  class_acc: 0.8695 (0.8649)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.0969 (13.1653)  time: 0.5172  data: 0.1728  max mem: 14356
Epoch: [46] Total time: 0:01:17 (0.8250 s / it)
Averaged stats: lr: 0.000278  min_lr: 0.000007  loss: 0.8336 (0.9474)  class_acc: 0.8695 (0.8649)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.0969 (13.1653)
[2024-07-17 07:54:26]  Epoch: [47]  [ 0/94]  eta: 0:08:02  lr: 0.000278  min_lr: 0.000007  loss: 1.0066 (1.0066)  class_acc: 0.8672 (0.8672)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.2195 (16.2195)  time: 5.1364  data: 4.7725  max mem: 14356
[2024-07-17 07:55:34]  Epoch: [47]  [93/94]  eta: 0:00:00  lr: 0.000270  min_lr: 0.000006  loss: 0.8655 (0.8709)  class_acc: 0.8672 (0.8731)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.7465 (13.3149)  time: 0.8367  data: 0.4928  max mem: 14356
Epoch: [47] Total time: 0:01:13 (0.7820 s / it)
Averaged stats: lr: 0.000270  min_lr: 0.000006  loss: 0.8655 (0.8709)  class_acc: 0.8672 (0.8731)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.7465 (13.3149)
[2024-07-17 07:55:41]  Epoch: [48]  [ 0/94]  eta: 0:10:31  lr: 0.000270  min_lr: 0.000006  loss: 0.7289 (0.7289)  class_acc: 0.8516 (0.8516)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.5523 (11.5523)  time: 6.7207  data: 6.3682  max mem: 14356
[2024-07-17 07:56:44]  Epoch: [48]  [93/94]  eta: 0:00:00  lr: 0.000262  min_lr: 0.000006  loss: 0.7997 (0.8484)  class_acc: 0.8758 (0.8768)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.7172 (13.0562)  time: 0.6973  data: 0.3522  max mem: 14356
Epoch: [48] Total time: 0:01:10 (0.7476 s / it)
Averaged stats: lr: 0.000262  min_lr: 0.000006  loss: 0.7997 (0.8484)  class_acc: 0.8758 (0.8768)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.7172 (13.0562)
[2024-07-17 07:56:51]  Epoch: [49]  [ 0/94]  eta: 0:09:42  lr: 0.000262  min_lr: 0.000006  loss: 1.0956 (1.0956)  class_acc: 0.8438 (0.8438)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.1239 (14.1239)  time: 6.2018  data: 5.8444  max mem: 14356
[2024-07-17 07:57:57]  Epoch: [49]  [93/94]  eta: 0:00:00  lr: 0.000255  min_lr: 0.000006  loss: 0.8607 (0.8378)  class_acc: 0.8762 (0.8775)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.4915 (12.7766)  time: 0.6207  data: 0.2776  max mem: 14356
Epoch: [49] Total time: 0:01:12 (0.7679 s / it)
Averaged stats: lr: 0.000255  min_lr: 0.000006  loss: 0.8607 (0.8378)  class_acc: 0.8762 (0.8775)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.4915 (12.7766)
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-49.pth
[2024-07-17 07:58:01]  Test:  [ 0/16]  eta: 0:01:02  loss: 20.8334 (20.8334)  acc: 0.8490 (0.8490)  recognition_fmeasure: 0.9580 (0.9580)  time: 3.9269  data: 3.2712  max mem: 14356
[2024-07-17 07:58:08]  Test:  [10/16]  eta: 0:00:05  loss: 20.2378 (20.2378)  acc: 0.7955 (0.7955)  recognition_fmeasure: 0.9067 (0.9067)  time: 0.9449  data: 0.2975  max mem: 14356
[2024-07-17 07:58:11]  Test:  [15/16]  eta: 0:00:00  loss: 20.4716 (20.4716)  acc: 0.8099 (0.8065)  recognition_fmeasure: 0.9169 (0.9140)  time: 0.8269  data: 0.2045  max mem: 14356
Test: Total time: 0:00:13 (0.8497 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.8065 loss 20.4716 Rec_fmeasure 0.9140
Accuracy of the network on the 2941 test images: 0.8065%
Max accuracy: 0.81%
[2024-07-17 07:58:18]  Epoch: [50]  [ 0/94]  eta: 0:10:49  lr: 0.000254  min_lr: 0.000006  loss: 1.0552 (1.0552)  class_acc: 0.8516 (0.8516)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.4543 (11.4543)  time: 6.9055  data: 6.5471  max mem: 14356
[2024-07-17 07:59:22]  Epoch: [50]  [93/94]  eta: 0:00:00  lr: 0.000247  min_lr: 0.000006  loss: 0.8264 (0.8149)  class_acc: 0.8750 (0.8807)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.2166 (12.7359)  time: 0.5370  data: 0.1938  max mem: 14356
Epoch: [50] Total time: 0:01:11 (0.7596 s / it)
Averaged stats: lr: 0.000247  min_lr: 0.000006  loss: 0.8264 (0.8149)  class_acc: 0.8750 (0.8807)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.2166 (12.7359)
[2024-07-17 07:59:28]  Epoch: [51]  [ 0/94]  eta: 0:08:44  lr: 0.000247  min_lr: 0.000006  loss: 0.5113 (0.5113)  class_acc: 0.9219 (0.9219)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.8333 (11.8333)  time: 5.5844  data: 5.2155  max mem: 14356
[2024-07-17 08:00:40]  Epoch: [51]  [93/94]  eta: 0:00:00  lr: 0.000239  min_lr: 0.000006  loss: 0.8251 (0.8122)  class_acc: 0.8789 (0.8799)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.9492 (12.8147)  time: 0.5516  data: 0.2088  max mem: 14356
Epoch: [51] Total time: 0:01:17 (0.8286 s / it)
Averaged stats: lr: 0.000239  min_lr: 0.000006  loss: 0.8251 (0.8122)  class_acc: 0.8789 (0.8799)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.9492 (12.8147)
[2024-07-17 08:00:44]  Epoch: [52]  [ 0/94]  eta: 0:05:47  lr: 0.000239  min_lr: 0.000006  loss: 0.3047 (0.3047)  class_acc: 0.9453 (0.9453)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2239 (8.2239)  time: 3.7012  data: 3.3277  max mem: 14356
[2024-07-17 08:01:58]  Epoch: [52]  [93/94]  eta: 0:00:00  lr: 0.000231  min_lr: 0.000005  loss: 0.7238 (0.7936)  class_acc: 0.8859 (0.8816)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.7304 (12.9239)  time: 0.6267  data: 0.2842  max mem: 14356
Epoch: [52] Total time: 0:01:18 (0.8329 s / it)
Averaged stats: lr: 0.000231  min_lr: 0.000005  loss: 0.7238 (0.7936)  class_acc: 0.8859 (0.8816)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.7304 (12.9239)
[2024-07-17 08:02:03]  Epoch: [53]  [ 0/94]  eta: 0:07:12  lr: 0.000231  min_lr: 0.000005  loss: 0.7885 (0.7885)  class_acc: 0.8750 (0.8750)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.5420 (12.5420)  time: 4.5989  data: 4.2372  max mem: 14356
[2024-07-17 08:03:08]  Epoch: [53]  [93/94]  eta: 0:00:00  lr: 0.000223  min_lr: 0.000005  loss: 0.7245 (0.7514)  class_acc: 0.8828 (0.8893)  loss_scale: 16384.0000 (19347.0638)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.8236 (inf)  time: 0.7268  data: 0.3826  max mem: 14356
Epoch: [53] Total time: 0:01:09 (0.7375 s / it)
Averaged stats: lr: 0.000223  min_lr: 0.000005  loss: 0.7245 (0.7514)  class_acc: 0.8828 (0.8893)  loss_scale: 16384.0000 (19347.0638)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.8236 (inf)
[2024-07-17 08:03:13]  Epoch: [54]  [ 0/94]  eta: 0:07:17  lr: 0.000223  min_lr: 0.000005  loss: 0.7985 (0.7985)  class_acc: 0.9141 (0.9141)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.1739 (12.1739)  time: 4.6578  data: 4.2998  max mem: 14356
[2024-07-17 08:04:21]  Epoch: [54]  [93/94]  eta: 0:00:00  lr: 0.000215  min_lr: 0.000005  loss: 0.6966 (0.7331)  class_acc: 0.8938 (0.8906)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.5470 (12.5984)  time: 0.7110  data: 0.3671  max mem: 14356
Epoch: [54] Total time: 0:01:12 (0.7751 s / it)
Averaged stats: lr: 0.000215  min_lr: 0.000005  loss: 0.6966 (0.7331)  class_acc: 0.8938 (0.8906)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.5470 (12.5984)
[2024-07-17 08:04:25]  Test:  [ 0/16]  eta: 0:00:58  loss: 20.8133 (20.8133)  acc: 0.8490 (0.8490)  recognition_fmeasure: 0.9634 (0.9634)  time: 3.6872  data: 3.0368  max mem: 14356
[2024-07-17 08:04:31]  Test:  [10/16]  eta: 0:00:05  loss: 20.2129 (20.2129)  acc: 0.8016 (0.8016)  recognition_fmeasure: 0.9093 (0.9093)  time: 0.9227  data: 0.2762  max mem: 14356
[2024-07-17 08:04:34]  Test:  [15/16]  eta: 0:00:00  loss: 20.4576 (20.4576)  acc: 0.8141 (0.8109)  recognition_fmeasure: 0.9185 (0.9156)  time: 0.8114  data: 0.1899  max mem: 14356
Test: Total time: 0:00:13 (0.8339 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.8109 loss 20.4576 Rec_fmeasure 0.9156
Accuracy of the network on the 2941 test images: 0.8109%
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-best.pth
Max accuracy: 0.81%
[2024-07-17 08:04:41]  Epoch: [55]  [ 0/94]  eta: 0:08:05  lr: 0.000215  min_lr: 0.000005  loss: 0.7170 (0.7170)  class_acc: 0.8984 (0.8984)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6376 (10.6376)  time: 5.1636  data: 4.8016  max mem: 14356
[2024-07-17 08:05:45]  Epoch: [55]  [93/94]  eta: 0:00:00  lr: 0.000207  min_lr: 0.000005  loss: 0.7505 (0.7303)  class_acc: 0.8926 (0.8912)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.1842 (12.7461)  time: 0.5257  data: 0.1805  max mem: 14356
Epoch: [55] Total time: 0:01:09 (0.7384 s / it)
Averaged stats: lr: 0.000207  min_lr: 0.000005  loss: 0.7505 (0.7303)  class_acc: 0.8926 (0.8912)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.1842 (12.7461)
[2024-07-17 08:05:52]  Epoch: [56]  [ 0/94]  eta: 0:10:56  lr: 0.000207  min_lr: 0.000005  loss: 0.9791 (0.9791)  class_acc: 0.8125 (0.8125)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.4471 (13.4471)  time: 6.9869  data: 6.6286  max mem: 14356
[2024-07-17 08:06:59]  Epoch: [56]  [93/94]  eta: 0:00:00  lr: 0.000199  min_lr: 0.000005  loss: 0.7058 (0.7079)  class_acc: 0.8930 (0.8931)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.1856 (12.2036)  time: 0.4919  data: 0.1419  max mem: 14356
Epoch: [56] Total time: 0:01:14 (0.7930 s / it)
Averaged stats: lr: 0.000199  min_lr: 0.000005  loss: 0.7058 (0.7079)  class_acc: 0.8930 (0.8931)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.1856 (12.2036)
[2024-07-17 08:07:05]  Epoch: [57]  [ 0/94]  eta: 0:08:44  lr: 0.000199  min_lr: 0.000005  loss: 0.6764 (0.6764)  class_acc: 0.8828 (0.8828)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7799 (10.7799)  time: 5.5799  data: 5.2141  max mem: 14356
[2024-07-17 08:08:20]  Epoch: [57]  [93/94]  eta: 0:00:00  lr: 0.000192  min_lr: 0.000005  loss: 0.6998 (0.7019)  class_acc: 0.8910 (0.8947)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.1169 (12.2382)  time: 0.6586  data: 0.3132  max mem: 14356
Epoch: [57] Total time: 0:01:20 (0.8585 s / it)
Averaged stats: lr: 0.000192  min_lr: 0.000005  loss: 0.6998 (0.7019)  class_acc: 0.8910 (0.8947)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.1169 (12.2382)
[2024-07-17 08:08:26]  Epoch: [58]  [ 0/94]  eta: 0:09:10  lr: 0.000192  min_lr: 0.000005  loss: 0.6688 (0.6688)  class_acc: 0.8906 (0.8906)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8164 (10.8164)  time: 5.8606  data: 5.5030  max mem: 14356
[2024-07-17 08:09:37]  Epoch: [58]  [93/94]  eta: 0:00:00  lr: 0.000184  min_lr: 0.000004  loss: 0.6172 (0.6670)  class_acc: 0.9062 (0.8976)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3543 (12.0932)  time: 0.7170  data: 0.3735  max mem: 14356
Epoch: [58] Total time: 0:01:16 (0.8185 s / it)
Averaged stats: lr: 0.000184  min_lr: 0.000004  loss: 0.6172 (0.6670)  class_acc: 0.9062 (0.8976)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3543 (12.0932)
[2024-07-17 08:09:42]  Epoch: [59]  [ 0/94]  eta: 0:07:02  lr: 0.000184  min_lr: 0.000004  loss: 0.6706 (0.6706)  class_acc: 0.9375 (0.9375)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.4224 (13.4224)  time: 4.4955  data: 4.1341  max mem: 14356
[2024-07-17 08:10:42]  Epoch: [59]  [93/94]  eta: 0:00:00  lr: 0.000176  min_lr: 0.000004  loss: 0.6903 (0.6458)  class_acc: 0.9023 (0.9010)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8718 (11.5425)  time: 0.7082  data: 0.3644  max mem: 14356
Epoch: [59] Total time: 0:01:04 (0.6864 s / it)
Averaged stats: lr: 0.000176  min_lr: 0.000004  loss: 0.6903 (0.6458)  class_acc: 0.9023 (0.9010)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8718 (11.5425)
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-59.pth
[2024-07-17 08:10:46]  Test:  [ 0/16]  eta: 0:01:00  loss: 20.7806 (20.7806)  acc: 0.8646 (0.8646)  recognition_fmeasure: 0.9591 (0.9591)  time: 3.7875  data: 3.1351  max mem: 14356
[2024-07-17 08:10:53]  Test:  [10/16]  eta: 0:00:05  loss: 20.2082 (20.2082)  acc: 0.8040 (0.8040)  recognition_fmeasure: 0.9093 (0.9093)  time: 0.9334  data: 0.2851  max mem: 14356
[2024-07-17 08:10:56]  Test:  [15/16]  eta: 0:00:00  loss: 20.4540 (20.4540)  acc: 0.8147 (0.8123)  recognition_fmeasure: 0.9179 (0.9153)  time: 0.8176  data: 0.1960  max mem: 14356
Test: Total time: 0:00:13 (0.8406 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.8123 loss 20.4540 Rec_fmeasure 0.9153
Accuracy of the network on the 2941 test images: 0.8123%
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-best.pth
Max accuracy: 0.81%
[2024-07-17 08:11:03]  Epoch: [60]  [ 0/94]  eta: 0:09:30  lr: 0.000176  min_lr: 0.000004  loss: 0.7061 (0.7061)  class_acc: 0.8828 (0.8828)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.6146 (11.6146)  time: 6.0706  data: 5.6968  max mem: 14356
[2024-07-17 08:12:11]  Epoch: [60]  [93/94]  eta: 0:00:00  lr: 0.000169  min_lr: 0.000004  loss: 0.5966 (0.6016)  class_acc: 0.9082 (0.9045)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3588 (11.4445)  time: 0.6694  data: 0.3220  max mem: 14356
Epoch: [60] Total time: 0:01:13 (0.7859 s / it)
Averaged stats: lr: 0.000169  min_lr: 0.000004  loss: 0.5966 (0.6016)  class_acc: 0.9082 (0.9045)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3588 (11.4445)
[2024-07-17 08:12:17]  Epoch: [61]  [ 0/94]  eta: 0:09:08  lr: 0.000169  min_lr: 0.000004  loss: 0.4559 (0.4559)  class_acc: 0.9219 (0.9219)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2193 (10.2193)  time: 5.8313  data: 5.4651  max mem: 14356
[2024-07-17 08:13:23]  Epoch: [61]  [93/94]  eta: 0:00:00  lr: 0.000162  min_lr: 0.000004  loss: 0.5939 (0.5974)  class_acc: 0.9090 (0.9062)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.0898 (11.3275)  time: 0.5243  data: 0.1806  max mem: 14356
Epoch: [61] Total time: 0:01:12 (0.7695 s / it)
Averaged stats: lr: 0.000162  min_lr: 0.000004  loss: 0.5939 (0.5974)  class_acc: 0.9090 (0.9062)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.0898 (11.3275)
[2024-07-17 08:13:31]  Epoch: [62]  [ 0/94]  eta: 0:11:45  lr: 0.000161  min_lr: 0.000004  loss: 0.5949 (0.5949)  class_acc: 0.8906 (0.8906)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.0191 (11.0191)  time: 7.5076  data: 7.1451  max mem: 14356
[2024-07-17 08:14:41]  Epoch: [62]  [93/94]  eta: 0:00:00  lr: 0.000154  min_lr: 0.000004  loss: 0.5570 (0.5833)  class_acc: 0.9035 (0.9059)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.2497 (11.3380)  time: 0.4836  data: 0.1398  max mem: 14356
Epoch: [62] Total time: 0:01:17 (0.8274 s / it)
Averaged stats: lr: 0.000154  min_lr: 0.000004  loss: 0.5570 (0.5833)  class_acc: 0.9035 (0.9059)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.2497 (11.3380)
[2024-07-17 08:14:46]  Epoch: [63]  [ 0/94]  eta: 0:07:46  lr: 0.000154  min_lr: 0.000004  loss: 0.5992 (0.5992)  class_acc: 0.8750 (0.8750)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.9821 (11.9821)  time: 4.9665  data: 4.6026  max mem: 14356
[2024-07-17 08:15:58]  Epoch: [63]  [93/94]  eta: 0:00:00  lr: 0.000147  min_lr: 0.000003  loss: 0.5673 (0.5647)  class_acc: 0.9113 (0.9080)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4322 (11.1980)  time: 0.7426  data: 0.3950  max mem: 14356
Epoch: [63] Total time: 0:01:16 (0.8181 s / it)
Averaged stats: lr: 0.000147  min_lr: 0.000003  loss: 0.5673 (0.5647)  class_acc: 0.9113 (0.9080)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4322 (11.1980)
[2024-07-17 08:16:03]  Epoch: [64]  [ 0/94]  eta: 0:06:45  lr: 0.000147  min_lr: 0.000003  loss: 0.1960 (0.1960)  class_acc: 0.9766 (0.9766)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7402 (5.7402)  time: 4.3093  data: 3.9517  max mem: 14356
[2024-07-17 08:17:08]  Epoch: [64]  [93/94]  eta: 0:00:00  lr: 0.000140  min_lr: 0.000003  loss: 0.5879 (0.5495)  class_acc: 0.9098 (0.9107)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.5676 (11.1119)  time: 0.8206  data: 0.4745  max mem: 14356
Epoch: [64] Total time: 0:01:09 (0.7441 s / it)
Averaged stats: lr: 0.000140  min_lr: 0.000003  loss: 0.5879 (0.5495)  class_acc: 0.9098 (0.9107)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.5676 (11.1119)
[2024-07-17 08:17:12]  Test:  [ 0/16]  eta: 0:00:55  loss: 20.8100 (20.8100)  acc: 0.8490 (0.8490)  recognition_fmeasure: 0.9552 (0.9552)  time: 3.4909  data: 2.8366  max mem: 14356
[2024-07-17 08:17:18]  Test:  [10/16]  eta: 0:00:05  loss: 20.2365 (20.2365)  acc: 0.8011 (0.8011)  recognition_fmeasure: 0.9071 (0.9071)  time: 0.9030  data: 0.2580  max mem: 14356
[2024-07-17 08:17:21]  Test:  [15/16]  eta: 0:00:00  loss: 20.4760 (20.4760)  acc: 0.8130 (0.8113)  recognition_fmeasure: 0.9173 (0.9148)  time: 0.7966  data: 0.1774  max mem: 14356
Test: Total time: 0:00:13 (0.8188 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.8113 loss 20.4760 Rec_fmeasure 0.9148
Accuracy of the network on the 2941 test images: 0.8113%
Max accuracy: 0.81%
[2024-07-17 08:17:27]  Epoch: [65]  [ 0/94]  eta: 0:08:47  lr: 0.000140  min_lr: 0.000003  loss: 0.4018 (0.4018)  class_acc: 0.9297 (0.9297)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7403 (9.7403)  time: 5.6118  data: 5.2546  max mem: 14356
[2024-07-17 08:18:28]  Epoch: [65]  [93/94]  eta: 0:00:00  lr: 0.000133  min_lr: 0.000003  loss: 0.5270 (0.5305)  class_acc: 0.9086 (0.9176)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9223 (11.2447)  time: 0.6981  data: 0.3494  max mem: 14356
Epoch: [65] Total time: 0:01:07 (0.7184 s / it)
Averaged stats: lr: 0.000133  min_lr: 0.000003  loss: 0.5270 (0.5305)  class_acc: 0.9086 (0.9176)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9223 (11.2447)
[2024-07-17 08:18:35]  Epoch: [66]  [ 0/94]  eta: 0:08:54  lr: 0.000133  min_lr: 0.000003  loss: 0.2655 (0.2655)  class_acc: 0.9297 (0.9297)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3125 (8.3125)  time: 5.6903  data: 5.3304  max mem: 14356
[2024-07-17 08:19:44]  Epoch: [66]  [93/94]  eta: 0:00:00  lr: 0.000126  min_lr: 0.000003  loss: 0.4847 (0.5011)  class_acc: 0.9219 (0.9146)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6938 (11.3190)  time: 0.4993  data: 0.1545  max mem: 14356
Epoch: [66] Total time: 0:01:15 (0.8071 s / it)
Averaged stats: lr: 0.000126  min_lr: 0.000003  loss: 0.4847 (0.5011)  class_acc: 0.9219 (0.9146)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6938 (11.3190)
[2024-07-17 08:19:50]  Epoch: [67]  [ 0/94]  eta: 0:08:20  lr: 0.000126  min_lr: 0.000003  loss: 0.5867 (0.5867)  class_acc: 0.9062 (0.9062)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.5343 (12.5343)  time: 5.3200  data: 4.9597  max mem: 14356
[2024-07-17 08:21:04]  Epoch: [67]  [93/94]  eta: 0:00:00  lr: 0.000119  min_lr: 0.000003  loss: 0.4598 (0.4905)  class_acc: 0.9270 (0.9230)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3124 (10.5441)  time: 0.5862  data: 0.2429  max mem: 14356
Epoch: [67] Total time: 0:01:19 (0.8462 s / it)
Averaged stats: lr: 0.000119  min_lr: 0.000003  loss: 0.4598 (0.4905)  class_acc: 0.9270 (0.9230)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3124 (10.5441)
[2024-07-17 08:21:09]  Epoch: [68]  [ 0/94]  eta: 0:06:40  lr: 0.000119  min_lr: 0.000003  loss: 0.3000 (0.3000)  class_acc: 0.9531 (0.9531)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9242 (8.9242)  time: 4.2578  data: 3.8975  max mem: 14356
[2024-07-17 08:22:24]  Epoch: [68]  [93/94]  eta: 0:00:00  lr: 0.000112  min_lr: 0.000003  loss: 0.4568 (0.4638)  class_acc: 0.9297 (0.9228)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.1328 (10.9370)  time: 0.6858  data: 0.3425  max mem: 14356
Epoch: [68] Total time: 0:01:19 (0.8503 s / it)
Averaged stats: lr: 0.000112  min_lr: 0.000003  loss: 0.4568 (0.4638)  class_acc: 0.9297 (0.9228)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.1328 (10.9370)
[2024-07-17 08:22:28]  Epoch: [69]  [ 0/94]  eta: 0:05:55  lr: 0.000112  min_lr: 0.000003  loss: 0.3112 (0.3112)  class_acc: 0.9609 (0.9609)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6404 (8.6404)  time: 3.7856  data: 3.4230  max mem: 14356
[2024-07-17 08:23:34]  Epoch: [69]  [93/94]  eta: 0:00:00  lr: 0.000106  min_lr: 0.000003  loss: 0.3921 (0.4459)  class_acc: 0.9320 (0.9236)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6812 (10.4929)  time: 0.7931  data: 0.4448  max mem: 14356
Epoch: [69] Total time: 0:01:10 (0.7485 s / it)
Averaged stats: lr: 0.000106  min_lr: 0.000003  loss: 0.3921 (0.4459)  class_acc: 0.9320 (0.9236)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6812 (10.4929)
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-69.pth
[2024-07-17 08:23:39]  Test:  [ 0/16]  eta: 0:01:00  loss: 20.7425 (20.7425)  acc: 0.8698 (0.8698)  recognition_fmeasure: 0.9622 (0.9622)  time: 3.7883  data: 3.1384  max mem: 14356
[2024-07-17 08:23:45]  Test:  [10/16]  eta: 0:00:05  loss: 20.2134 (20.2134)  acc: 0.8101 (0.8101)  recognition_fmeasure: 0.9097 (0.9097)  time: 0.9309  data: 0.2855  max mem: 14356
[2024-07-17 08:23:48]  Test:  [15/16]  eta: 0:00:00  loss: 20.4524 (20.4524)  acc: 0.8179 (0.8164)  recognition_fmeasure: 0.9170 (0.9145)  time: 0.8175  data: 0.1963  max mem: 14356
Test: Total time: 0:00:13 (0.8373 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.8164 loss 20.4524 Rec_fmeasure 0.9145
Accuracy of the network on the 2941 test images: 0.8164%
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-best.pth
Max accuracy: 0.82%
[2024-07-17 08:23:53]  Epoch: [70]  [ 0/94]  eta: 0:04:29  lr: 0.000106  min_lr: 0.000003  loss: 0.5647 (0.5647)  class_acc: 0.9062 (0.9062)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.6272 (11.6272)  time: 2.8694  data: 2.5120  max mem: 14356
[2024-07-17 08:25:07]  Epoch: [70]  [93/94]  eta: 0:00:00  lr: 0.000099  min_lr: 0.000002  loss: 0.4179 (0.4311)  class_acc: 0.9266 (0.9297)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4270 (10.7686)  time: 0.8439  data: 0.4994  max mem: 14356
Epoch: [70] Total time: 0:01:16 (0.8187 s / it)
Averaged stats: lr: 0.000099  min_lr: 0.000002  loss: 0.4179 (0.4311)  class_acc: 0.9266 (0.9297)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4270 (10.7686)
[2024-07-17 08:25:12]  Epoch: [71]  [ 0/94]  eta: 0:07:27  lr: 0.000099  min_lr: 0.000002  loss: 0.7183 (0.7183)  class_acc: 0.8984 (0.8984)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.0531 (15.0531)  time: 4.7633  data: 4.3976  max mem: 14356
[2024-07-17 08:26:13]  Epoch: [71]  [93/94]  eta: 0:00:00  lr: 0.000093  min_lr: 0.000002  loss: 0.4080 (0.4405)  class_acc: 0.9262 (0.9248)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7842 (10.3454)  time: 0.7753  data: 0.4224  max mem: 14356
Epoch: [71] Total time: 0:01:06 (0.7114 s / it)
Averaged stats: lr: 0.000093  min_lr: 0.000002  loss: 0.4080 (0.4405)  class_acc: 0.9262 (0.9248)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7842 (10.3454)
[2024-07-17 08:26:19]  Epoch: [72]  [ 0/94]  eta: 0:08:13  lr: 0.000093  min_lr: 0.000002  loss: 0.3738 (0.3738)  class_acc: 0.9141 (0.9141)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.6489 (11.6489)  time: 5.2499  data: 4.8756  max mem: 14356
[2024-07-17 08:27:29]  Epoch: [72]  [93/94]  eta: 0:00:00  lr: 0.000087  min_lr: 0.000002  loss: 0.4053 (0.4165)  class_acc: 0.9309 (0.9292)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6056 (10.4612)  time: 0.5876  data: 0.2459  max mem: 14356
Epoch: [72] Total time: 0:01:15 (0.8066 s / it)
Averaged stats: lr: 0.000087  min_lr: 0.000002  loss: 0.4053 (0.4165)  class_acc: 0.9309 (0.9292)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6056 (10.4612)
[2024-07-17 08:27:34]  Epoch: [73]  [ 0/94]  eta: 0:06:30  lr: 0.000087  min_lr: 0.000002  loss: 0.3967 (0.3967)  class_acc: 0.9375 (0.9375)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1047 (9.1047)  time: 4.1567  data: 3.7807  max mem: 14356
[2024-07-17 08:28:48]  Epoch: [73]  [93/94]  eta: 0:00:00  lr: 0.000081  min_lr: 0.000002  loss: 0.3502 (0.3948)  class_acc: 0.9371 (0.9314)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4935 (10.1756)  time: 0.5485  data: 0.2060  max mem: 14356
Epoch: [73] Total time: 0:01:18 (0.8331 s / it)
Averaged stats: lr: 0.000081  min_lr: 0.000002  loss: 0.3502 (0.3948)  class_acc: 0.9371 (0.9314)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4935 (10.1756)
[2024-07-17 08:28:51]  Epoch: [74]  [ 0/94]  eta: 0:05:08  lr: 0.000081  min_lr: 0.000002  loss: 0.3810 (0.3810)  class_acc: 0.9531 (0.9531)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3951 (8.3951)  time: 3.2849  data: 2.9156  max mem: 14356
[2024-07-17 08:30:03]  Epoch: [74]  [93/94]  eta: 0:00:00  lr: 0.000076  min_lr: 0.000002  loss: 0.4052 (0.4100)  class_acc: 0.9281 (0.9284)  loss_scale: 32768.0000 (25273.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9255 (10.2925)  time: 0.7448  data: 0.3993  max mem: 14356
Epoch: [74] Total time: 0:01:15 (0.7982 s / it)
Averaged stats: lr: 0.000076  min_lr: 0.000002  loss: 0.4052 (0.4100)  class_acc: 0.9281 (0.9284)  loss_scale: 32768.0000 (25273.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9255 (10.2925)
[2024-07-17 08:30:07]  Test:  [ 0/16]  eta: 0:00:56  loss: 20.7933 (20.7933)  acc: 0.8490 (0.8490)  recognition_fmeasure: 0.9565 (0.9565)  time: 3.5554  data: 2.9013  max mem: 14356
[2024-07-17 08:30:13]  Test:  [10/16]  eta: 0:00:05  loss: 20.2185 (20.2185)  acc: 0.8063 (0.8063)  recognition_fmeasure: 0.9101 (0.9101)  time: 0.9119  data: 0.2639  max mem: 14356
[2024-07-17 08:30:16]  Test:  [15/16]  eta: 0:00:00  loss: 20.4559 (20.4559)  acc: 0.8173 (0.8157)  recognition_fmeasure: 0.9180 (0.9156)  time: 0.8041  data: 0.1814  max mem: 14356
Test: Total time: 0:00:13 (0.8179 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.8157 loss 20.4559 Rec_fmeasure 0.9156
Accuracy of the network on the 2941 test images: 0.8157%
Max accuracy: 0.82%
[2024-07-17 08:30:20]  Epoch: [75]  [ 0/94]  eta: 0:06:29  lr: 0.000075  min_lr: 0.000002  loss: 0.4327 (0.4327)  class_acc: 0.8984 (0.8984)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.1040 (12.1040)  time: 4.1453  data: 3.7790  max mem: 14356
[2024-07-17 08:31:37]  Epoch: [75]  [93/94]  eta: 0:00:00  lr: 0.000070  min_lr: 0.000002  loss: 0.4166 (0.3801)  class_acc: 0.9348 (0.9331)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2758 (9.9326)  time: 0.6147  data: 0.2690  max mem: 14356
Epoch: [75] Total time: 0:01:21 (0.8634 s / it)
Averaged stats: lr: 0.000070  min_lr: 0.000002  loss: 0.4166 (0.3801)  class_acc: 0.9348 (0.9331)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2758 (9.9326)
[2024-07-17 08:31:42]  Epoch: [76]  [ 0/94]  eta: 0:07:46  lr: 0.000070  min_lr: 0.000002  loss: 0.2455 (0.2455)  class_acc: 0.9453 (0.9453)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0641 (8.0641)  time: 4.9622  data: 4.5905  max mem: 14356
[2024-07-17 08:32:54]  Epoch: [76]  [93/94]  eta: 0:00:00  lr: 0.000065  min_lr: 0.000002  loss: 0.3941 (0.4100)  class_acc: 0.9230 (0.9267)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0922 (10.4456)  time: 0.8232  data: 0.4789  max mem: 14356
Epoch: [76] Total time: 0:01:17 (0.8244 s / it)
Averaged stats: lr: 0.000065  min_lr: 0.000002  loss: 0.3941 (0.4100)  class_acc: 0.9230 (0.9267)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0922 (10.4456)
[2024-07-17 08:32:58]  Epoch: [77]  [ 0/94]  eta: 0:04:50  lr: 0.000065  min_lr: 0.000002  loss: 0.1870 (0.1870)  class_acc: 0.9531 (0.9531)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7636 (6.7636)  time: 3.0928  data: 2.7291  max mem: 14356
[2024-07-17 08:34:01]  Epoch: [77]  [93/94]  eta: 0:00:00  lr: 0.000059  min_lr: 0.000001  loss: 0.3613 (0.3854)  class_acc: 0.9340 (0.9331)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8750 (10.1702)  time: 0.7063  data: 0.3589  max mem: 14356
Epoch: [77] Total time: 0:01:06 (0.7085 s / it)
Averaged stats: lr: 0.000059  min_lr: 0.000001  loss: 0.3613 (0.3854)  class_acc: 0.9340 (0.9331)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8750 (10.1702)
[2024-07-17 08:34:08]  Epoch: [78]  [ 0/94]  eta: 0:09:44  lr: 0.000059  min_lr: 0.000001  loss: 0.8476 (0.8476)  class_acc: 0.8672 (0.8672)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.5740 (13.5740)  time: 6.2176  data: 5.8529  max mem: 14356
[2024-07-17 08:35:15]  Epoch: [78]  [93/94]  eta: 0:00:00  lr: 0.000054  min_lr: 0.000001  loss: 0.3867 (0.3948)  class_acc: 0.9391 (0.9335)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5291 (10.1258)  time: 0.5975  data: 0.2555  max mem: 14356
Epoch: [78] Total time: 0:01:13 (0.7850 s / it)
Averaged stats: lr: 0.000054  min_lr: 0.000001  loss: 0.3867 (0.3948)  class_acc: 0.9391 (0.9335)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5291 (10.1258)
[2024-07-17 08:35:20]  Epoch: [79]  [ 0/94]  eta: 0:07:13  lr: 0.000054  min_lr: 0.000001  loss: 0.1440 (0.1440)  class_acc: 0.9766 (0.9766)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0396 (7.0396)  time: 4.6089  data: 4.2464  max mem: 14356
[2024-07-17 08:36:31]  Epoch: [79]  [93/94]  eta: 0:00:00  lr: 0.000050  min_lr: 0.000001  loss: 0.3261 (0.3569)  class_acc: 0.9410 (0.9402)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4603 (9.5787)  time: 0.5511  data: 0.2068  max mem: 14356
Epoch: [79] Total time: 0:01:16 (0.8134 s / it)
Averaged stats: lr: 0.000050  min_lr: 0.000001  loss: 0.3261 (0.3569)  class_acc: 0.9410 (0.9402)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4603 (9.5787)
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-79.pth
[2024-07-17 08:36:35]  Test:  [ 0/16]  eta: 0:00:51  loss: 20.8288 (20.8288)  acc: 0.8594 (0.8594)  recognition_fmeasure: 0.9601 (0.9601)  time: 3.2040  data: 2.5450  max mem: 14356
[2024-07-17 08:36:42]  Test:  [10/16]  eta: 0:00:05  loss: 20.2092 (20.2092)  acc: 0.8092 (0.8092)  recognition_fmeasure: 0.9121 (0.9121)  time: 0.8785  data: 0.2315  max mem: 14356
[2024-07-17 08:36:45]  Test:  [15/16]  eta: 0:00:00  loss: 20.4541 (20.4541)  acc: 0.8226 (0.8198)  recognition_fmeasure: 0.9205 (0.9179)  time: 0.7800  data: 0.1591  max mem: 14356
Test: Total time: 0:00:12 (0.8038 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.8198 loss 20.4541 Rec_fmeasure 0.9179
Accuracy of the network on the 2941 test images: 0.8198%
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-best.pth
Max accuracy: 0.82%
[2024-07-17 08:36:52]  Epoch: [80]  [ 0/94]  eta: 0:08:02  lr: 0.000050  min_lr: 0.000001  loss: 0.3681 (0.3681)  class_acc: 0.9297 (0.9297)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.1603 (12.1603)  time: 5.1377  data: 4.7805  max mem: 14356
[2024-07-17 08:38:00]  Epoch: [80]  [93/94]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 0.3524 (0.3347)  class_acc: 0.9434 (0.9393)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6281 (9.5955)  time: 0.4790  data: 0.1379  max mem: 14356
Epoch: [80] Total time: 0:01:13 (0.7871 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 0.3524 (0.3347)  class_acc: 0.9434 (0.9393)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6281 (9.5955)
[2024-07-17 08:38:06]  Epoch: [81]  [ 0/94]  eta: 0:08:26  lr: 0.000045  min_lr: 0.000001  loss: 0.1733 (0.1733)  class_acc: 0.9531 (0.9531)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6214 (6.6214)  time: 5.3919  data: 5.0246  max mem: 14356
[2024-07-17 08:39:20]  Epoch: [81]  [93/94]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 0.3477 (0.3603)  class_acc: 0.9391 (0.9383)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9440 (10.0487)  time: 0.5588  data: 0.2130  max mem: 14356
Epoch: [81] Total time: 0:01:19 (0.8490 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 0.3477 (0.3603)  class_acc: 0.9391 (0.9383)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9440 (10.0487)
[2024-07-17 08:39:25]  Epoch: [82]  [ 0/94]  eta: 0:08:03  lr: 0.000041  min_lr: 0.000001  loss: 0.6012 (0.6012)  class_acc: 0.8906 (0.8906)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.5829 (11.5829)  time: 5.1462  data: 4.7799  max mem: 14356
[2024-07-17 08:40:34]  Epoch: [82]  [93/94]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000001  loss: 0.3092 (0.3296)  class_acc: 0.9422 (0.9411)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0789 (9.6256)  time: 0.7092  data: 0.3638  max mem: 14356
Epoch: [82] Total time: 0:01:14 (0.7901 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000001  loss: 0.3092 (0.3296)  class_acc: 0.9422 (0.9411)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0789 (9.6256)
[2024-07-17 08:40:41]  Epoch: [83]  [ 0/94]  eta: 0:10:18  lr: 0.000036  min_lr: 0.000001  loss: 0.3416 (0.3416)  class_acc: 0.9531 (0.9531)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7073 (11.7073)  time: 6.5789  data: 6.2068  max mem: 14356
[2024-07-17 08:41:40]  Epoch: [83]  [93/94]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 0.3129 (0.3255)  class_acc: 0.9492 (0.9452)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0899 (9.3204)  time: 0.5482  data: 0.2050  max mem: 14356
Epoch: [83] Total time: 0:01:05 (0.6990 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 0.3129 (0.3255)  class_acc: 0.9492 (0.9452)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0899 (9.3204)
[2024-07-17 08:41:45]  Epoch: [84]  [ 0/94]  eta: 0:07:07  lr: 0.000032  min_lr: 0.000001  loss: 0.3013 (0.3013)  class_acc: 0.9375 (0.9375)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8328 (9.8328)  time: 4.5500  data: 4.1857  max mem: 14356
[2024-07-17 08:42:31]  Epoch: [84]  [93/94]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 0.3404 (0.3290)  class_acc: 0.9422 (0.9426)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7293 (10.0604)  time: 0.3548  data: 0.0102  max mem: 14356
Epoch: [84] Total time: 0:00:51 (0.5445 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 0.3404 (0.3290)  class_acc: 0.9422 (0.9426)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7293 (10.0604)
[2024-07-17 08:42:33]  Test:  [ 0/16]  eta: 0:00:30  loss: 20.8462 (20.8462)  acc: 0.8646 (0.8646)  recognition_fmeasure: 0.9579 (0.9579)  time: 1.9272  data: 1.2650  max mem: 14356
[2024-07-17 08:42:40]  Test:  [10/16]  eta: 0:00:04  loss: 20.2109 (20.2109)  acc: 0.8120 (0.8120)  recognition_fmeasure: 0.9133 (0.9133)  time: 0.7644  data: 0.1151  max mem: 14356
[2024-07-17 08:42:43]  Test:  [15/16]  eta: 0:00:00  loss: 20.4472 (20.4472)  acc: 0.8222 (0.8208)  recognition_fmeasure: 0.9214 (0.9190)  time: 0.7021  data: 0.0792  max mem: 14356
Test: Total time: 0:00:11 (0.7208 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.8208 loss 20.4472 Rec_fmeasure 0.9190
Accuracy of the network on the 2941 test images: 0.8208%
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-best.pth
Max accuracy: 0.82%
[2024-07-17 08:42:50]  Epoch: [85]  [ 0/94]  eta: 0:08:19  lr: 0.000029  min_lr: 0.000001  loss: 0.5157 (0.5157)  class_acc: 0.8984 (0.8984)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.3668 (13.3668)  time: 5.3096  data: 4.9418  max mem: 14356
[2024-07-17 08:43:31]  Epoch: [85]  [93/94]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000001  loss: 0.2861 (0.3163)  class_acc: 0.9492 (0.9433)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1770 (9.6399)  time: 0.3456  data: 0.0001  max mem: 14356
Epoch: [85] Total time: 0:00:46 (0.4941 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000001  loss: 0.2861 (0.3163)  class_acc: 0.9492 (0.9433)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1770 (9.6399)
[2024-07-17 08:43:36]  Epoch: [86]  [ 0/94]  eta: 0:07:45  lr: 0.000025  min_lr: 0.000001  loss: 0.3862 (0.3862)  class_acc: 0.9297 (0.9297)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8241 (8.8241)  time: 4.9484  data: 4.5882  max mem: 14356
[2024-07-17 08:44:21]  Epoch: [86]  [93/94]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 0.3390 (0.3112)  class_acc: 0.9406 (0.9437)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4476 (9.2424)  time: 0.3485  data: 0.0001  max mem: 14356
Epoch: [86] Total time: 0:00:50 (0.5350 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 0.3390 (0.3112)  class_acc: 0.9406 (0.9437)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4476 (9.2424)
[2024-07-17 08:44:24]  Epoch: [87]  [ 0/94]  eta: 0:04:45  lr: 0.000022  min_lr: 0.000001  loss: 0.2732 (0.2732)  class_acc: 0.9531 (0.9531)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5837 (10.5837)  time: 3.0338  data: 2.6747  max mem: 14356
[2024-07-17 08:45:16]  Epoch: [87]  [93/94]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 0.3230 (0.3094)  class_acc: 0.9430 (0.9443)  loss_scale: 16384.0000 (24576.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6158 (inf)  time: 0.4717  data: 0.1244  max mem: 14356
Epoch: [87] Total time: 0:00:54 (0.5839 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 0.3230 (0.3094)  class_acc: 0.9430 (0.9443)  loss_scale: 16384.0000 (24576.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6158 (inf)
[2024-07-17 08:45:18]  Epoch: [88]  [ 0/94]  eta: 0:03:44  lr: 0.000019  min_lr: 0.000000  loss: 0.4414 (0.4414)  class_acc: 0.9219 (0.9219)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9065 (8.9065)  time: 2.3911  data: 2.0256  max mem: 14356
[2024-07-17 08:46:09]  Epoch: [88]  [93/94]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 0.3045 (0.2933)  class_acc: 0.9477 (0.9471)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0196 (9.4460)  time: 0.4422  data: 0.0975  max mem: 14356
Epoch: [88] Total time: 0:00:53 (0.5704 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 0.3045 (0.2933)  class_acc: 0.9477 (0.9471)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0196 (9.4460)
[2024-07-17 08:46:13]  Epoch: [89]  [ 0/94]  eta: 0:04:56  lr: 0.000016  min_lr: 0.000000  loss: 0.3133 (0.3133)  class_acc: 0.9375 (0.9375)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9990 (9.9990)  time: 3.1557  data: 2.7888  max mem: 14356
[2024-07-17 08:46:56]  Epoch: [89]  [93/94]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 0.3230 (0.3327)  class_acc: 0.9457 (0.9423)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6526 (10.1295)  time: 0.5662  data: 0.2177  max mem: 14356
Epoch: [89] Total time: 0:00:46 (0.4997 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 0.3230 (0.3327)  class_acc: 0.9457 (0.9423)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6526 (10.1295)
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-89.pth
[2024-07-17 08:47:00]  Test:  [ 0/16]  eta: 0:00:46  loss: 20.8296 (20.8296)  acc: 0.8698 (0.8698)  recognition_fmeasure: 0.9676 (0.9676)  time: 2.9091  data: 2.2594  max mem: 14356
[2024-07-17 08:47:07]  Test:  [10/16]  eta: 0:00:05  loss: 20.2175 (20.2175)  acc: 0.8149 (0.8149)  recognition_fmeasure: 0.9141 (0.9141)  time: 0.8513  data: 0.2055  max mem: 14356
[2024-07-17 08:47:09]  Test:  [15/16]  eta: 0:00:00  loss: 20.4538 (20.4538)  acc: 0.8254 (0.8242)  recognition_fmeasure: 0.9220 (0.9196)  time: 0.7625  data: 0.1413  max mem: 14356
Test: Total time: 0:00:12 (0.7773 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.8242 loss 20.4538 Rec_fmeasure 0.9196
Accuracy of the network on the 2941 test images: 0.8242%
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-best.pth
Max accuracy: 0.82%
[2024-07-17 08:47:14]  Epoch: [90]  [ 0/94]  eta: 0:04:45  lr: 0.000013  min_lr: 0.000000  loss: 0.2654 (0.2654)  class_acc: 0.9453 (0.9453)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4065 (7.4065)  time: 3.0369  data: 2.6682  max mem: 14356
[2024-07-17 08:48:05]  Epoch: [90]  [93/94]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 0.3087 (0.3083)  class_acc: 0.9414 (0.9439)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6101 (9.3153)  time: 0.4328  data: 0.0845  max mem: 14356
Epoch: [90] Total time: 0:00:54 (0.5765 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 0.3087 (0.3083)  class_acc: 0.9414 (0.9439)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6101 (9.3153)
[2024-07-17 08:48:09]  Epoch: [91]  [ 0/94]  eta: 0:06:07  lr: 0.000011  min_lr: 0.000000  loss: 0.2684 (0.2684)  class_acc: 0.9609 (0.9609)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8996 (7.8996)  time: 3.9122  data: 3.5523  max mem: 14356
[2024-07-17 08:48:59]  Epoch: [91]  [93/94]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 0.2993 (0.2945)  class_acc: 0.9457 (0.9456)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4634 (9.4940)  time: 0.4283  data: 0.0853  max mem: 14356
Epoch: [91] Total time: 0:00:54 (0.5785 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 0.2993 (0.2945)  class_acc: 0.9457 (0.9456)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4634 (9.4940)
[2024-07-17 08:49:03]  Epoch: [92]  [ 0/94]  eta: 0:04:45  lr: 0.000009  min_lr: 0.000000  loss: 0.3457 (0.3457)  class_acc: 0.9062 (0.9062)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0699 (10.0699)  time: 3.0333  data: 2.6694  max mem: 14356
[2024-07-17 08:49:51]  Epoch: [92]  [93/94]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 0.2674 (0.2732)  class_acc: 0.9477 (0.9466)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3457 (8.9172)  time: 0.3963  data: 0.0547  max mem: 14356
Epoch: [92] Total time: 0:00:51 (0.5506 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 0.2674 (0.2732)  class_acc: 0.9477 (0.9466)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3457 (8.9172)
[2024-07-17 08:49:54]  Epoch: [93]  [ 0/94]  eta: 0:04:44  lr: 0.000007  min_lr: 0.000000  loss: 0.3900 (0.3900)  class_acc: 0.9297 (0.9297)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7397 (10.7397)  time: 3.0274  data: 2.6680  max mem: 14356
[2024-07-17 08:50:43]  Epoch: [93]  [93/94]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 0.2934 (0.2958)  class_acc: 0.9465 (0.9468)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8693 (9.4474)  time: 0.5356  data: 0.1924  max mem: 14356
Epoch: [93] Total time: 0:00:52 (0.5556 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 0.2934 (0.2958)  class_acc: 0.9465 (0.9468)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8693 (9.4474)
[2024-07-17 08:50:47]  Epoch: [94]  [ 0/94]  eta: 0:04:53  lr: 0.000006  min_lr: 0.000000  loss: 0.3396 (0.3396)  class_acc: 0.9375 (0.9375)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6791 (10.6791)  time: 3.1200  data: 2.7656  max mem: 14356
[2024-07-17 08:51:24]  Epoch: [94]  [93/94]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 0.3441 (0.3004)  class_acc: 0.9430 (0.9471)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9061 (9.1541)  time: 0.5536  data: 0.2107  max mem: 14356
Epoch: [94] Total time: 0:00:40 (0.4348 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 0.3441 (0.3004)  class_acc: 0.9430 (0.9471)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9061 (9.1541)
[2024-07-17 08:51:27]  Test:  [ 0/16]  eta: 0:00:46  loss: 20.8017 (20.8017)  acc: 0.8750 (0.8750)  recognition_fmeasure: 0.9673 (0.9673)  time: 2.8896  data: 2.2374  max mem: 14356
[2024-07-17 08:51:34]  Test:  [10/16]  eta: 0:00:05  loss: 20.1995 (20.1995)  acc: 0.8149 (0.8149)  recognition_fmeasure: 0.9146 (0.9146)  time: 0.8519  data: 0.2035  max mem: 14356
[2024-07-17 08:51:37]  Test:  [15/16]  eta: 0:00:00  loss: 20.4397 (20.4397)  acc: 0.8264 (0.8245)  recognition_fmeasure: 0.9233 (0.9207)  time: 0.7637  data: 0.1399  max mem: 14356
Test: Total time: 0:00:12 (0.7824 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.8245 loss 20.4397 Rec_fmeasure 0.9207
Accuracy of the network on the 2941 test images: 0.8245%
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-best.pth
Max accuracy: 0.82%
[2024-07-17 08:51:43]  Epoch: [95]  [ 0/94]  eta: 0:07:24  lr: 0.000004  min_lr: 0.000000  loss: 0.2411 (0.2411)  class_acc: 0.9688 (0.9688)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2765 (6.2765)  time: 4.7241  data: 4.3615  max mem: 14356
[2024-07-17 08:52:21]  Epoch: [95]  [93/94]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 0.2831 (0.2845)  class_acc: 0.9465 (0.9496)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0788 (9.0329)  time: 0.5364  data: 0.1913  max mem: 14356
Epoch: [95] Total time: 0:00:43 (0.4588 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 0.2831 (0.2845)  class_acc: 0.9465 (0.9496)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0788 (9.0329)
[2024-07-17 08:52:26]  Epoch: [96]  [ 0/94]  eta: 0:07:17  lr: 0.000003  min_lr: 0.000000  loss: 0.4942 (0.4942)  class_acc: 0.9297 (0.9297)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.2240 (11.2240)  time: 4.6557  data: 4.2887  max mem: 14356
[2024-07-17 08:53:07]  Epoch: [96]  [93/94]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 0.2674 (0.2688)  class_acc: 0.9582 (0.9515)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3858 (8.7349)  time: 0.3449  data: 0.0001  max mem: 14356
Epoch: [96] Total time: 0:00:45 (0.4861 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 0.2674 (0.2688)  class_acc: 0.9582 (0.9515)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3858 (8.7349)
[2024-07-17 08:53:12]  Epoch: [97]  [ 0/94]  eta: 0:07:46  lr: 0.000002  min_lr: 0.000000  loss: 0.3854 (0.3854)  class_acc: 0.9297 (0.9297)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5335 (8.5335)  time: 4.9576  data: 4.5918  max mem: 14356
[2024-07-17 08:53:58]  Epoch: [97]  [93/94]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 0.2405 (0.2805)  class_acc: 0.9555 (0.9502)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5908 (9.1101)  time: 0.3451  data: 0.0001  max mem: 14356
Epoch: [97] Total time: 0:00:51 (0.5458 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 0.2405 (0.2805)  class_acc: 0.9555 (0.9502)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5908 (9.1101)
[2024-07-17 08:54:02]  Epoch: [98]  [ 0/94]  eta: 0:04:54  lr: 0.000002  min_lr: 0.000000  loss: 0.1920 (0.1920)  class_acc: 0.9609 (0.9609)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8001 (7.8001)  time: 3.1372  data: 2.7787  max mem: 14356
[2024-07-17 08:54:52]  Epoch: [98]  [93/94]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 0.3219 (0.3120)  class_acc: 0.9449 (0.9441)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6196 (9.5960)  time: 0.3959  data: 0.0538  max mem: 14356
Epoch: [98] Total time: 0:00:54 (0.5769 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 0.3219 (0.3120)  class_acc: 0.9449 (0.9441)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6196 (9.5960)
[2024-07-17 08:54:57]  Epoch: [99]  [ 0/94]  eta: 0:07:06  lr: 0.000001  min_lr: 0.000000  loss: 0.3187 (0.3187)  class_acc: 0.9453 (0.9453)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3718 (9.3718)  time: 4.5344  data: 4.1662  max mem: 14356
[2024-07-17 08:55:45]  Epoch: [99]  [93/94]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 0.2949 (0.2965)  class_acc: 0.9488 (0.9479)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2393 (9.2202)  time: 0.4327  data: 0.0883  max mem: 14356
Epoch: [99] Total time: 0:00:52 (0.5580 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 0.2949 (0.2965)  class_acc: 0.9488 (0.9479)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2393 (9.2202)
saving to output/real_lmdb_base_batch128_lr1e-3/checkpoint-99.pth
[2024-07-17 08:55:48]  Test:  [ 0/16]  eta: 0:00:37  loss: 20.8032 (20.8032)  acc: 0.8698 (0.8698)  recognition_fmeasure: 0.9671 (0.9671)  time: 2.3144  data: 1.6571  max mem: 14356
[2024-07-17 08:55:55]  Test:  [10/16]  eta: 0:00:04  loss: 20.1998 (20.1998)  acc: 0.8130 (0.8130)  recognition_fmeasure: 0.9137 (0.9137)  time: 0.8011  data: 0.1508  max mem: 14356
[2024-07-17 08:55:57]  Test:  [15/16]  eta: 0:00:00  loss: 20.4395 (20.4395)  acc: 0.8235 (0.8222)  recognition_fmeasure: 0.9222 (0.9198)  time: 0.7270  data: 0.1037  max mem: 14356
Test: Total time: 0:00:11 (0.7418 s / it)
* /home/ysjeong/workspace/OCR/DiG/LMDB_test: 2941 images, Acc 0.8222 loss 20.4395 Rec_fmeasure 0.9198
Accuracy of the network on the 2941 test images: 0.8222%
Max accuracy: 0.82%
Training time 2:00:25
